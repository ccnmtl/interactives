
[
{
   "url": "/articles/standalone-interactives/",
   "title": "Packaging Javascript Interactives",
   "author": "Susan Dreher",
   "date": "",
   "content": "[Background At CTL, client-side interactives enrich many of our serial-learning web applications. These discrete Javascript blocks challenge students with quizzes, animations, case studies, calculators and games. The goal is to transform a passive reading exercise into an active learning experience. The interactives encourage students to use higher-order skills to deepen understanding and aid retention. Our in-house content management tool Pagetree provides the framework for our interactives. The Javascript blocks hook into the content hierarchy using well-known patterns. The Pagetree infrastructure is a powerful and cost-effective approach when user data must be collected and analyzed. But, many of our interactives carry enough context to stand on their own statelessly. Our clients often want to make this class of content available in blogs, wikis and social media. Our developers began exploring a way to make this happen years ago, but failed to find a reasonable solution. I was recently re-tasked with finding a way to make this happen. Research Luckily, we&amp;rsquo;re now not the only ones who want to organize, share and reuse Javascript code. Javascript build and release tools have matured and proliferated. npm organizes code into packages, and offers a repository for those packages. On install, npm downloads the package code and its dependencies. Webpack is a module bundler that can package both scripts and other assets such as stylesheets and images. Browserify, Require.js, jspm.io, rollup.js are similar module bundlers. The Web Components standard offers a way &amp;ldquo;to create reusable web components that include both HTML and JavaScript.&amp;rdquo; The choices are now overwhelming. Here are just a few comparisons to review: 1, 2, 3. In considering the options, I decided the Webpack module bundler was the best fit due to these winning features: Custom loaders. Our interactives need static data, images, stylesheets and clientside templates. The interactives rely on libraries like jQuery, Backbone.js and Bootstrap. Webpack&amp;rsquo;s custom loaders integrate all these things. A single line of code loads a json blob into a Backbone collection. Another line applies styles to the document. Easy. Development server. Webpack includes a local server with hot reload capabilities. The bundle is rebuilt and reloaded in the web page as code and assets change. One tool. Other tools require ancillary tools such as Grunt, Gulp, Bower and Babel to do all the things Webpack does. Dealing with just a single tool is quite appealing. Implementation My first implementation targeted an existing interactive in PASS, a website to educate pre-doctoral dental students about patient populations. The interactive demonstrates support services commonly available to older adults. The code, templates and static assets transferred over directly. The src/index.js loads the required libraries based, applies the stylesheet and creates the view. The webpack.config.js governs how the various file types are loaded. Package Structure npm and Webpack do not dictate a standard package structure. After a little research, I came up with this based on our needs. project |-- dist | |-- bundle.js | |-- *.svg, *.eot, *.woff*, *.ttf |-- node_modules |-- src | |-- index.js | |-- app-specific.js |-- static | |-- css | |-- img | |-- json |-- test | |-- test.webpack.config.js | |-- model-test.js | |-- view-test.js | |-- view-test.html |-- Makefile |-- index.html |-- package.json |-- webpack.config.js Makefile Anders, a senior developer here, composed Makefiles for our Django projects. I wanted to do the same here for consistency. The pack&amp;rsquo;s Makefile can build the bundle, run the dev server and publish the project. Per our team&amp;rsquo;s standards, jshint and jscs are also run. Static Data Many of our interactives rely on a database for static data. That data is usually fetched directly from the server and loaded into a Backbone collection. To replicate this flow, I generated a json file via Django&amp;rsquo;s dumpdata method. The json file was then loaded into the Backbone collection. Summary The interactive migration coalesced into a published package with minimal pain. Our long-term goal is to create an interactives gallery to allow our work to be reviewed and embedded. For now, I&amp;rsquo;m on to a more complex interactive migration to validate this approach. Look for a second post on composing and running unit tests and browser tests for a Webpack.]",
   "tags": ["javascript", "html5", "testing"]
},
{
   "url": "/articles/salt-to-slack/",
   "title": "Salt to Slack",
   "author": "Anders Pearson",
   "date": "",
   "content": "[While we aren&amp;rsquo;t entirely on the chatops bandwagon at CTL (yet), I do believe that visibility is important to operations. It&amp;rsquo;s often extremely helpful to know what&amp;rsquo;s going on with our systems at a glance. We have a #monitoring Slack channel that Github and Travis CI as well as our internal Jenkins server publish into so we can quickly see when pull requests come in or are merged, when tests fail, and when deployments are running. If you&amp;rsquo;re working on an application, you may see a pull request go by that looks like it might conflict with what you&amp;rsquo;re working on and you can back out some changes before you get too far. Or if you&amp;rsquo;re thinking of deploying some code, but you see that there are already a bunch of deployments happening, you might decide to hold off for a bit. One area where we lacked visibility and occasionally ran into conflicts were configuration management runs. We use Salt for all of our configuration management and orchestration. With Salt, like most other CM systems, you edit files to define the overall configuration of your infrastructure, then you run a command with basically says &amp;ldquo;update the servers to match the config&amp;rdquo;. In Salt&amp;rsquo;s case, that is done by running a &amp;ldquo;highstate&amp;ldquo; command on the Salt master server. That works fine as long as only one person is making changes and running highstates. If two people are trying to do it at the same time without coordinating, it gets confusing fast. It&amp;rsquo;s also the sort of thing that you&amp;rsquo;d like to know about if you&amp;rsquo;re deploying applications. In the course of a highstate, Salt may install packages and restart services left and right. This can cause deployments to fail in strange ways and you can spend a lot of time debugging if you didn&amp;rsquo;t know that a highstate happened while you were deploying. Running a highstate also sometimes has unintended consequences. A typo in a config file might break something for a different application in a non-obvious way. Our monitoring systems should detect that, but there can be a delay and we still have large gaps in our monitoring. The Slack state files are tracked in git so we have a nice history and audit trail there, but you still don&amp;rsquo;t know exactly when the changes you see in git were applied via a highstate, so post-facto debugging can still be a chore. Salt also lets you run arbitrary commands across machines, which is handy for, eg, restarting a service that&amp;rsquo;s acting up. Those commands can change the state of the servers and further complicate debugging. So I really wanted to make Salt highstates and commands visible in our Slack channel to provide a basic audit log and improve coordination. To implement this, I made use of Salt&amp;rsquo;s Reactor System. The reactor system lets you trigger arbitrary commands from Salt&amp;rsquo;s internal events. You configure Salt Reactor by putting an /etc/salt/master.d/reactor.conf file on the salt master that tells it to map classes of events to handler. We use reactors for a few other things as well, but the part that&amp;rsquo;s relevant here looks like: reactor: - &#39;salt/job/*/new&#39;: - /srv/reactor/slack.sls That tells Salt to pass every new job event (basically anything that is run manually) to the slack.sls reactor. The reactor is a bit trickier with an ugly mix of YAML and Jinja syntax: {% if data[&#39;fun&#39;] == &#39;state.highstate&#39; %} slack-highstate: local.cmd.run: - tgt: saltmaster - arg: - /usr/local/bin/salt_slack {{data[&#39;tgt&#39;]}} {{data[&#39;tgt_type&#39;]}} {{data[&#39;fun&#39;]}} {{data[&#39;arg&#39;]}} {% endif %} {% if data[&#39;fun&#39;] == &#39;state.sls&#39; %} slack-state: local.cmd.run: - tgt: saltmaster - arg: - /usr/local/bin/salt_slack {{data[&#39;tgt&#39;]}} {{data[&#39;tgt_type&#39;]}} {{data[&#39;fun&#39;]}} {{data[&#39;arg&#39;]}} {% endif %} {% if data[&#39;fun&#39;] == &#39;cmd.run&#39; and data[&#39;tgt&#39;] != &#39;saltmaster&#39; %} slack-highstate: local.cmd.run: - tgt: saltmaster - arg: - /usr/local/bin/salt_slack {{data[&#39;tgt&#39;]}} {{data[&#39;tgt_type&#39;]}} {{data[&#39;fun&#39;]}} {{data[&#39;arg&#39;]}} {% endif %} The three event subtypes that we want to handle are state.highstate, state.sls, and cmd.run. Whenever one of those is seen, it pulls out a few fields of data from the event and runs a salt_slack command on the salt master with those fields as arguments. The cmd.run stanza has a very important conditional on it (data[&#39;tgt&#39;] != &#39;saltmaster&#39;). That tells it to ignore cmd.run events running on the salt master itself. That&amp;rsquo;s important because the salt_slack command is run via cmd.run. Triggering another cmd.run every time it sees a cmd.run would cause a nice infinite loop. I can tell you from experience, an infinite loop on your salt master is not a fun time. Finally, the salt_slack command is a little Python script that turns the data from the events back into something intelligible and sends it to Slack&amp;rsquo;s webhook: #!/usr/bin/env python import requests import json import sys ENDPOINT = &amp;quot;https://hooks.slack.com/services/&amp;lt;slack token goes here&amp;gt;&amp;quot; channel = &amp;quot;#monitoring&amp;quot; emoji = &amp;quot;:computer:&amp;quot; target = sys.argv[1] target_type = sys.argv[2] fun = sys.argv[3] args = &amp;quot;&amp;quot; if len(sys.argv) &amp;gt; 4: args = sys.argv[4] args = args.strip(&amp;quot;[]&amp;quot;) def deformat(target, target_type): if target_type == &amp;quot;glob&amp;quot;: return target if target_type == &amp;quot;grain&amp;quot;: return &amp;quot;-G &amp;quot; &#43; target return target command = &amp;quot;salt %s %s %s&amp;quot; % (deformat(target, target_type), fun, args) payload = dict( channel=channel, mrkdwn=True, username=&amp;quot;salt-bot&amp;quot;, icon_emoji=emoji, attachments=[ { &amp;quot;mrkdwn_in&amp;quot;: [&amp;quot;text&amp;quot;, &amp;quot;fallback&amp;quot;], &amp;quot;fallback&amp;quot;: command, &amp;quot;text&amp;quot;: &amp;quot;`&amp;quot; &#43; command &#43; &amp;quot;`&amp;quot;, &amp;quot;color&amp;quot;: &amp;quot;#F35A00&amp;quot; } ] ) data = dict( payload=json.dumps(payload) ) r = requests.post(ENDPOINT, data=data) Now, when someone runs salt &#39;*&#39; state.highstate, salt -G roles:postgresql state.sls, or salt -G roles:nginx cmd.run &#39;nginx restart&#39;, those commands are immediately posted to our Slack channel for everyone to see.]",
   "tags": ["salt", "slack", "chatops"]
},
{
   "url": "/articles/migrating-mediathreads-collection-bookmarklet-to-browser-extensions/",
   "title": "Migrating Mediathread&#39;s Collection Bookmarklet to Browser Extensions",
   "author": "Nik Nyby",
   "date": "",
   "content": "[In the past few years, a web standard called Content Security Policy has come up that allows web developers to restrict how media and code can be accessed on their website depending on where the assets are being served. GitHub and Mozilla both have further explanations of CSP. Part of our Mediathread application involves collecting images, audio, and video files from around the web and bringing them in to Mediathread for annotation or use in compositions. We accomplished this by providing a bookmarklet that looks for media on the user&amp;rsquo;s web page and imports it into Mediathread when the user selects it. As web sites implement CSP mechanisms, there&amp;rsquo;s a strong possibility of preventing external JavaScript from getting executed, which includes code in our bookmarklet. We decided to develop browser extensions for Chrome, Firefox, and Safari to replace the bookmarklet. The JavaScript that runs in browser extensions is considered privileged code that can&amp;rsquo;t be disabled outright via CSP. The bookmarklet&amp;rsquo;s code contains intricate logic for collecting assets in site-specific ways. The bookmarklet&amp;rsquo;s behavior can be recreated in the extension using content scripts. Chrome, Firefox, and Safari all have a mechanisms for content scripts. Safari calls them injected scripts. Because content scripts have access to the DOM, just like the old bookmarklet code expects, I was able to adapt the existing codebase to an &amp;ldquo;extension-compatible&amp;rdquo; version, and our three Mediathread extensions all share the core common code. I adapted this core code for the extensions into the mediathread-collect repository. Last November I wrote about some options we have around Managing Common Code in a Multi-Browser Extension. Mozilla is working on a project called WebExtensions in an effort to standardize some APIs available to browser extensions, and make it easier to port Chrome extensions to Firefox. So cross-browser extensions may become simpler in the future, but there will still be plenty of kinks to work out, especially for Safari and Microsoft Edge compatibility. We&amp;rsquo;ve received positive feedback about the extension, and it&amp;rsquo;s simpler to install than the bookmarklet. We&amp;rsquo;ll be maintaining these extensions along with Mediathread, and I&amp;rsquo;ll continue to look for ways to simplify the process of working with the cross-browser codebase.]",
   "tags": ["javascript", "browser-extensions", "mediathread"]
},
{
   "url": "/articles/rebuilding-compiled/",
   "title": "Rebuilding CompilED",
   "author": "Zarina Mustapha",
   "date": "",
   "content": "[&amp;ldquo;We can rebuild [it]. We have the technology.&amp;rdquo; — Oscar Goldman, The Six Million Dollar Man Earlier in March we decided to move our previous blog to a different platform, with a fresh design in hope to breathe new life into it, and keep it going painlessly. CompilED has been online since 2009 on the erstwhile gratis and open-source Movable Type (MT), a weblog publishing system developed by the company Six Apart. Over the years, it became increasingly difficult to upgrade and maintain the Movable Type platform in our server. When Six Apart changed its business model and terminated the open source licensing, we ceased to upgrade the platform and began looking for other options that would meet our changing requirements for static publishing tools at the Center. For the new publishing engine, we would like to: Separate content and templates for better workflow, Use version control tools to track changes to the content and codes, and allow rollbacks, Store the source on Github and use its interface for content editing, Have control over functionalities we can develop for the site, Have clean codes, Have a framework that is lightweight and easy to maintain or upgrade, at little or no cost. Conversion and infrastructure Our programmer Anders has been testing and evaluating static site generators for the Center, and recommended Hugo for sites that we published on Movable Type. Hugo is a general-purpose static site generator that renders HTML files as output from content files and layout templates. It doesn’t rely on a database; all the content is in simple text files with Markdown or limited HTML formatting that can be revised using any text editing tool. Hugo meets the requirements we’ve set earlier, and we’ve used it for the Film Language Glossary and the Case Consortium in 2015. It seemed to be a ideal fit for CompilED, and we wanted to explore this framework&amp;rsquo;s flexibility and durability. Anders wrote a Python scraper that took all the content from the previous CompilED and converted it into the formatting suited for Hugo. Then, he set up the compiled repository on Github, as well as the continuous integration server, Jenkins, to deploy the site to staging and production. CompilED is published to and hosted on Amazon Simple Storage Service (Amazon S3). Architecture, theming, and skinning Initially, the conversion preserved the original directory structure from Movable Type. Each article post lived on its respective topic directory: . ├── content │ ├── events │ │ ├── article-post-files │ ├── process │ │ ├── article-post-files │ ├── projects │ │ ├── article-post-files │ ├── etcetera... │ │ ├── article-post-files This structure made it difficult for me to use the existing Hugo list of content templates, layout template assigments, and other functionality such as global pagination. It also complicated the editorial process where contributors would have to drop their posts in a topic directory and move the files around if the topic changed. To simplify content and template management, I eliminated the topic directories, and turned the topics into taxanomies that are defined in each article&amp;rsquo;s front matter. Here, an article&amp;rsquo;s topic can be edited, deleted, or created at anytime without moving the file between directories. All posts are now in one directory, articles. This is the new content structure: . ├── content │ ├── articles │ │ ├── article-post-files Here&amp;rsquo;s a snippet of an article&amp;rsquo;s front matter: --- author_name: &amp;quot;Zarina Mustapha&amp;quot; date: 2016-05-19 title: &amp;quot;Rebuilding CompilED&amp;quot; topics: - Process type: post --- I used the Bootstrap framework to develop the skin. For CompilED, I implemented the site design in layouts/ instead of creating a sharable skin in themes/. While this will limit the reuse of the the skin in another Hugo site, converting the design into a theme for future use will not be difficult. Homepage, before (on desktop) Homepage, after (on desktop) Article page, before (on desktop) Article page, after (on desktop) Access and accessibility The older CompilED site was designed and developed with little accommodations for mobile devices or assistive technology (AT) for web accessibility. One of my goals is to make the site accessible to everyone, independent of the devices they use. For the new CompilED, I set out to implement comprehensive responsive design, from mobile to desktop, using the Bootstrap framework. Article page, before (on iPhone 6) Article page, after (on iPhone 6) In addition to this, I adhered closely to common web standards (HTML5, semantic elements) so that the site is accessible to those using assistive technologies. While my experience in developing and designing fully accessible sites is in the early stages, I put in a good faith effort to make CompilED work with VoiceOver for OSX and TalkBack for Android tablets. The site passed reasonably well when tested with WebAIM&amp;rsquo;s web accessibility evaluation tool, where the errors the site encountered are from yet-to-be-improved functionalities. More will be written soon about what I&amp;rsquo;ve learned in making sites for access and accessibility. Structure, search, and share Hugo, similar to Movable Type v4.x, doesn&amp;rsquo;t come with a seach functionality. We could have set up a Google custom site search like we did in the old CompilED, but, according to Anders, &amp;ldquo;that would&amp;rsquo;ve limited our ability to customize the display and integration of search results and there would always be a delay between changes to content being made and Google re-indexing the site.&amp;rdquo; Anders implemented lunr.js, client-side full text and faceted search engine. lunr.js loads a JSON file containing all of the content for the entire site and running the search algorithm directly in the browser rather than back on a server. At the moment, the scale of content in CompilED is small enough for us to use lunr.js. The content in CompilED is highly structured using the schema.org vocabulary, so that it can be recognized, organized and displayed accordingly by seach engines. Social media sharing of articles is markedly improved also in the new site. What&amp;rsquo;s the deal with the kitties? We work A few more to do Quisque lobortis eget metus sit amet elementum. Integer in leo magna. Praesent ullamcorper convallis porta. Maecenas bibendum nunc rhoncus, lobortis lectus eu, venenatis leo. Nam porttitor lorem eu mi consectetur cursus. Duis venenatis sapien ac tellus rhoncus tincidunt. In neque ante, ultrices quis tortor et, pulvinar porta nibh. Nam in.]",
   "tags": ["hugo", "html5", "accessibility", "responsive design", "structured data", "microdata"]
},
{
   "url": "/articles/access-accessibility/",
   "title": "Access and Accessibility",
   "author": "Zarina Mustapha",
   "date": "",
   "content": "[Catnipsum dolor sit amet sleep on the sofa sleepy kitty catnip sleep on your keyboard catnip. To look at the queen molly chartreux catnip tabby catnip miaow. Jump catnipsum catnip catnap sylvester. Catnip rolly angora grooming. Burmilla mmmeoww hair ball pet me jump warm kitty i&amp;rsquo;ve been to london whiskers. Pouncing pet me toy laze in the sun fur catnip siamese cat mmmeoww. Catnap catnipsum look what the cat dragged in mreeewww rowwrrr. Cat-eyes smokey siamese dappled fur soft fur puss tail. Hiss milo kittens hanging from the screens roll in the grass gizmo molly pouncing mmmeoww. Calico catnipsum rolling catnipsum max. Sylvester there&amp;rsquo;s a kitten in my shoe alley cat stretching garfield sleep on the sofa boots catnip. Stuck in a tree catnipsum where have u been patch purring softly long thin tail shiny coat mouse. Meew persian ginger i don&amp;rsquo;t believe we just have two fish kittens hanging from the screens miaowww catnip catnip. Longhair cat sunbathe meaaaawww fluffy a cat has nine lives catnip mmeww. Kitty-litter midnight hiss taz cuddle me persian. Kittens kittens everywhere fat cat cat and mouse act sleep on the sofa fat cat persian feathers. Mewmew pussy cat, pussy cat fur fluffy. Will they do this when they&amp;rsquo;re older pussy cat, pussy cat cat got your tounge laze in the sun persian meew cat. Rolly daisy american longhair ragdoll sassy tigger sniffing cat and mouse act. Climb the curtain mmmeoww cool cat kittens hanging from the screens stuck in a tree clawing. Squeaking mice jumping catnip catnap boots pussy. Catnip hairball to look at the queen thai. Mrowwwww cuddle me ragdoll scratching miioo-oo-oo siberian. There&amp;rsquo;s a kitten in the bread kittens hanging from the screens siamese tiger little ball of fur sniffing calico.]",
   "tags": []
},
{
   "url": "/articles/hugo-review/",
   "title": "Hugo: A View from the Front-end",
   "author": "Zarina Mustapha",
   "date": "",
   "content": "[]",
   "tags": []
},
{
   "url": "/articles/teaching-digital-humanities-on-a-laptop/",
   "title": "Teaching Digital Humanities on A Laptop",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[This summer Graham Sack, a doctoral student in the English department is teaching an introductory course in Digital Humanities called &amp;ldquo;Computational Methods for Literary and Cultural Criticism&amp;rdquo;. Graham came to CCNMTL inquiring about the usage of a cutting edge approach to teaching programing to novices, a web-based programming environment called IPython Notebook IPython Notebook is a tool that runs in your browser and allows for the full execution of any Python program that can run on the underlying server. Similar to a wave of new web-based educational programming environments, such as Code Academy and specialized tools that run within MOOC platforms like Coursera and EdX, IPython Notebook allows users to author and execute programs through their browser without requiring them to interact with the file system, text editors, and command lines. IPython Notebook also allows for the creation of annotations and commentary so that the code can be interspersed with blocks of formatted text. It can also be configured to display mathematical equations, rich media and charts inline, so that the results of numerical computations can be displayed visually, directly in the browser. All of these elements can be combined in a single, portable document that can be shared and modified by anyone running IPython Notebook. This is a gallery of some interesting notebooks and a video of the notebook in action: Video: IPython Notebook has become very popular in scientific research communities as a tool for communicating and publishing methods and calculations. It is a powerful way for researchers to create a research notebook, showing their work and demonstrating to their peers exactly how they arrived at a particular solution. Monumental calculation errors such as the Reinhart and Rogoff spreadsheet bug, where the accidental omission of a few rows in a calculation resulted in a the IMF introducing austerity policies in Europe, demonstrate the increasing importance of data provenance, and the ability for researchers to show their work so that others can reproduce and validate the results. Tools like IPython Notebook point towards a format that these kinds of publication might take. Education is another area where IPython Notebook is becoming popular. Instead of using a fixed slide deck, and perhaps an interactive terminal session for demonstrations, classroom lecture notes are created within IPython Notebook allowing the teacher to interactively modify the examples and then re-rerun them to explain a concept or respond to a question. The IPython Notebook documents can also be distributed to students so that they can follow along with the examples in class, or review them afterwards. Within their copy of the IPython Notebook document, students can also modify and tweak the code, allowing them to poke at the parts they want to explore. Assignments can also be distributed as IPython Notebook documents which the students can modify with their responses, submit to their teacher, who can in turn provide them with feedback, directly within the context of their code. All by exchanging small, portable .ipynb files. Graham was very interested in trying to teach with this tool, but needed some advice on the best way to enable his students to access this tool. A startup called trinket.io now offers web-based interactive tutorials as a service, although they do not currently provide access to all of the libraries that Graham wanted to teach with. Graham also had other challenges to overcome, more general than providing access to this specific tool, and increasingly common among non-STEM faculty trying to introduce programming to their students. He needed a secure way to provide his students with a programming environment, and one that was fully stocked with all of the libraries and tools that he wanted them to learn about. In Graham&amp;rsquo;s case, he wanted to expose them to a wide range of very powerful libraries, such as Python&amp;rsquo;s Natural Language Processing Toolkit (NLTK, Topic Modeling (gensim), Data Analyis (pandas), Scientific Computing (NumPy), Text Processing (TextBlob), and graphical plotting (matplotlib). These libraries can be quite difficult to install, and though the experience of setting them up is itself instructive, teaching systems administration skills was not Graham&amp;rsquo;s learning objective and he wanted his students to have a uniform, pre-built environment so they could immediately start using these powerful tools. There are a few pre-built ipython notebook environments for windows/mac, such as Anaconda or Enthought, but, once again, they don&amp;rsquo;t ship with the libraries that Graham wanted to teach with, and the installation process was complex and daunting. Currently, neither Columbia&amp;rsquo;s Central IT department nor the Libraries has a standard solution to the problem of setting up secure custom computing environments for instruction. As more and more courses across the social sciences and humanities embrace computational methods, there is a growing demand for services that address this need. Together with Graham, CCNMTL developed a two-pronged solution for his course. First, we developed a solution that would enable his students to each run their own environment, locally on their own computers. IPython Notebook runs as a web server, and we needed to come up with a cross-platform solution that would allow us to easily distribute and update the complex custom environment that Graham needed. Using a popular tool that open-source communities have been adopting to create uniform development environments, we built a custom Vagrant box which contained a fully configured IPython Notebook server along with pre-built versions all of the libraries that Graham wanted to teach with. This Vagrant box was built on top of a Linux (Ubuntu 14.04) server, and setup to run within the VirtualBox virtualization software. This means that once the students install VirtualBox and Vagrant (which both support Windows, OSX, and Linux), the experience for the students was consistent. Graham created simple installation instructions (the zip file you need to download is here: DH-METHODS.zip), and we also released all of our work to Github. Second, as a fallback, in case some students had difficulty installing the software, we also prepared a server version of the software. Initially, we weren&amp;rsquo;t certain how many students would enroll in the class, and a server version would inevitably be more difficult to administer, in terms of securing access to the server and the filesystem. IPython Notebook is not (yet) a multiuser environment, so we knew we would need one notebook per student. Given these requirements, the Docker containment technology seemed like a good fit. Alongside the Vagrant image we also brought up a Docker version of an almost identical environment alongside. The class met for the first time last week, and after a few installation hiccups almost all the the students have successfully installed the Vagrant box. One or two have older machines that run the virtualized operating system too slowly to be practical, and we are working to set them up with their own Docker-based IPython Notebooks. A little less convenient than having all the tools local, but should get us through the summer. In the future we hope to also work with some of the folks running computer labs to have this software installed on computers available to the students. Finally, we&amp;rsquo;re hoping to connect with the DH-in-a-box community who are working in a very similar problem-space. With the growing need for secure, easy to use, computing platforms in teaching and learning, solutions like these are very interesting and important to explore.]",
   "tags": ["dh", "digital humanities", "docker", "ipython-notebook", "python", "vagrant"]
},
{
   "url": "/articles/reflections-on-the-open-analytics-summit/",
   "title": "Reflections on the Open Analytics Summit",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[Back in June, I attended the one-day Open Analytics Summit. We aren&#39;t really doing much with analytics or big data here at CCNMTL (yet), but there are many conversations and projects happening around campus and I wanted to get a better sense of the kinds of value these methods are yielding. These issues are sure to be central to much of the research and instruction at the Institute for Data Sciences and Engineering, and have already crept up on a number of Columbia projects we have been involved with, such as the Declassification Engine and the Open Syllabus Project. The conference was interesting, but I was a bit puzzled by the format. The talks were all 15 or 30 min in length, and the speakers rarely left any time for questions. It was almost like a day of long lightning talks - they talks weren&#39;t really long enough to get into too much depth, but I did get a flavor for the kind of work happening in this field. Some of the conference highlights: Nearly everyone is using hadoop. No real surprises there. I saw an impressive demo of elasticsearch (distributed SOLR), combined with logstash (which we are now using) and web-based querying tool - kibana. See demo.kibana.com and this writeup and video to see how this is used to query the twitter firehose. It&#39;s interesting to think about the different kinds of data that resemble log formats, and can be coaxed into this style of analysis. Apache Drill, a FOSS implementation of Google&#39;s Dremel, for using sql to query multiple data stores, including nosql ones. The speaker quipped that the apache foundation is borrowing its roadmap from Google&#39;s whitepapers. DataNitro - I thought this was super cool, even though its not open (though it is gratis for students) and windows-only. Basically treats excel as a front-end client (or, the View in an MVC system) for interacting with server-side python, and includes a python interpreter inside of excel for manipulating data. Looked really powerful for teaching, with plenty of IPython overlap, but has a pretty well defined niche. The author hopes that tools like these might do a better job with provenance, and prevent data disasters like the Reinhart &amp;amp; Rogoff depression. Luigi - (pycon talk) is a tool &#34;for batch data processing including dependency resolution and monitoring&#34;. It will be interesting to compare this to CCNMTL&#39;s Wardenclyffe (soon to be released!), a web-based, workflow orchestration tool that we use for batch processing of videos, and more. Chartbeat a service that allows sites to track in minute detail where users are spending time on their pages. Their software sends data back to chartbeat every second to let them know how long you have spent on the page, and where you mouse is pointing. An interesting finding is that once you eliminate users that leave a page right away, most users spend most of their time scrolled part-way down the page. Finally, I saw a fascinating talk about how big data and predictive modeling were used in the Obama campaign to strategize their media buys - I am pretty sure some of this was covered in the press, but the presenters were part of the campaign and shared some juicy details (like, how they spent something like 400k on &#34;set top box&#34; data), Here is their prezi presentation. They claimed that these techniques resulted in the Obama campaign spending $100/TV min/voter less than the Romney campaign. Overall, this summit was a pretty interesting mixture of sectors and tools. It wasn&#39;t quite as technical as I was hoping, and the format prevented anyone from diving into the detail I was hoping for. I was also left wondering what kind of real value this kind of analytics is providing, but there were a few examples in marketing that demonstrated the payoff, and everyone in this room believes enough in these methods to invest reams of resources into finding out that answer.]",
   "tags": ["analytics", "big data", "drill", "elasticsearch", "logstashd", "python"]
},
{
   "url": "/articles/ricon-east-2013-talk-summaries/",
   "title": "Ricon East 2013 Talk Summaries",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[Back in May, Anders attended the Ricon East, &#34;a distributed systems conference by and for engineers, developers, scientists and architects&#34;. The distributed data-store Riak was featured prominently at the conference but the event was intended more as a conference on distributed systems in general spanning academia and industry. Anders wrote up some fantastic, detailed notes on his personal blog, summarizing and explaining the sessions he attended: Ricon East 2013 Talk Summaries The slides and videos of the event are now posted, so you can check them out for yourself too.]",
   "tags": ["conference", "distributed", "nosql", "riak"]
},
{
   "url": "/articles/reliefsim-on-google-app-engine/",
   "title": "ReliefSim on Google App Engine",
   "author": "Anders Pearson",
   "date": "",
   "content": "[As an experiment, Anders has ported the ReliefSim application to Google&amp;rsquo;s AppEngine and gotten it running on the free version at reliefsim.appspot.com (the source code for this application has been released on github: github.com/ccnmtl/reliefsim). Brief Technical History of ReliefSim Eric Mattes wrote the original version of ReliefSim as a text-based unix console application. It would run on an account on one of our servers that users could ssh or telnet to and interact with via keyboard commands. ReliefSim has a very detailed model underneath it but has a very simple turn-based interaction model. Each turn, the user assigns each of their &#34;workers&#34; a task, which will take one or more turns to complete. Those tasks involve either gathering information (doing different types of health surveys, etc.) on the camp&#39;s population, or working to improve camp conditions in some way (improving food/water supply, giving vaccinations, etc). The original text-based version was completely stateful. The simulation maintained everything in memory and was shut down when the user disconnected. A few years back, CCNMTL contracted an external company to build a web interface to ReliefSim. Our technology stack at the time was TurboGears, so the app was built on that framework. In order to minimize the amount of work involved in the port, the web application was structured in a way that would involve the fewest changes to the underlying simulation as possible. Essentially, the main objects that held the game state were serialized (with Python&#39;s built-in &#39;pickle&#39; capabilities) on each turn and stored in TurboGears&#39; in-memory session storage. Since TG runs as a single, long-lived process, it meant that game state could be maintained for as long as the server process was running, but would be lost on a restart. This wasn&#39;t ideal, but saved us so much work that it was deemed an acceptable compromise. Of course, for the last few years, TurboGears has not been our preferred development/deployment platform and has become increasingly difficult for us to deal with. We&#39;ve kept our stable of TG apps running but try to treat them as legacy systems. ReliefSim, in particular, being built on TG, and not having any active clients at Columbia making use of it is hard to justify spending much developer time on. Nevertheless, we are proud of our little simulation and would like to keep it around in some form, at least for demonstration purposes. We&#39;ve also been looking at various external application/platform hosting services as ways to reduce our sysadmin load, and to give external organizations an easier path towards running their own instances of our web applications. Since the actual &#34;web application&#34; part of ReliefSim is quite small, and the expected traffic is very low, I felt that it made an interesting candidate for attempting to get it running on Google App Engine (GAE). Existing Architecture ReliefSim consists of a simulation module (contained in simulation.py), which exposes Simulation and WebUI classes, respectively containing the main game logic and helpers for adapting game state to the web framework. The TG component exposes &#34;/&#34;, &#34;/new&#34;, &#34;/turn&#34;, &#34;/execute&#34;, &#34;/data&#34; and &#34;/quit&#34;, &#34;/game_over&#34;, and &#34;/loadData&#34; URLs. Typical flow through the application is that the user visits &#34;/&#34;, is prompted to begin a new game, which they initiate by a POST request to &#34;/new&#34;. That creates new Simulation and WebUI objects, saves them to the built-in user session store, then redirects the user to &#34;/turn&#34;. &#34;/turn&#34; just renders an HTML template and control is handed off to some Javascript libraries which handle the user-interaction of the game. As the user performs actions in the game, Javascript makes POST requests to &#34;/execute&#34;, which retrieves the simulation/ui from the user&#39;s session, processes the actions performed, and returns data to the javascript to update the user interface. Eventually, the user &#34;dies&#34; or quits the game and gets sent to the &#34;/game_over&#34; or &#34;/quit&#34; page, both of which just display a message and offer to let the user download their data in csv format (via &#34;/data&#34;). Meanwhile, during game play, if the user turns on contextual help, hovering over a game element triggers a GET request to &#34;/loadData&#34; to retrieve information about that element which is stored in an XML file. Porting Porting to GAE involved replacing only the TG component of ReliefSim. The Simulation module, the Javascript and (most) of the HTML templates were left unchanged. GAE&#39;s webapp2 framework makes it very straightforward to associate a class/method with a URL, so making dummy views to respond to &#34;/&#34;, &#34;/new&#34;, &#34;/turn&#34;, &#34;/execute&#34;, etc was very simple. Things got more interesting quickly when it came time to actually persist game state between HTTP requests. This is where TG and GAE start to have fundamentally different world-views. Where we could be sure that our TG app ran as a single process and all threads had access to the same shared session store, GAE explicitly does NOT make that same guarantee. Since it is designed to run on Google&#39;s big farms of servers, it&#39;s very clear that any application state set up in one request will not be accessible on subsequent requests since it will possibly (or probably, even) not be the same process or even the same server handling each request. GAE doesn&#39;t even provide a session store out of the box. webapp2 has one available in an extensions module, but that stores everything in cookies, which is unacceptable for serializations of large game state objects. Instead, I had to take a more direct route and use the GAE datastore to store serialized game state objects in the database, effectively implementing my own little database backed session store. When the user hits &#34;/new&#34;, a new id is generated, a game started, serialized, and stored to a state object in the datastore as a binary blob under that id. The id is tucked away in a cookie and used to retrieve the state object from the datastore on subsequent hits. The next issue I hit was that GAE has a hard limit of 1MB on the request size for any operation, including saving data to the datastore. I quickly discovered that the game state for ReliefSim when pickled as we&#39;d been doing it is in the 4-8MB range. I experimented a bit and found that with zlib, I could compress it down to around 750k, and squeeze it into the datastore. Templates were then fairly simple to port. I pulled in jinja2 and stripped out the .kid xml-isms and replaced them with a very simple block/extends structure that should be familiar to our django apps. There were actually only one or two places where variables in templates were handled with kid and had to be ported to jinja2. Most of the page manipulation is done through javascript and was left untouched. The only other not-completely-straightforward thing I had to do to get it working was that GAE, for whatever reason (probably security restrictions), refused to let me parse the help information XML file directly off disk. It was a fairly small XML file though, so I just dropped it inline as a string directly in the Python source and was done with it. I completed the port, up to the point documented here in about 8 hours total development time. In fairness, a couple of those hours were spent doing a flake8 cleanup of the simulation module to get it in line with our other codebases, and this work was not strictly necessary for the port. The code for the GAE version is up on our CCNMTL github account: github.com/ccnmtl/reliefsim Concerns From a bit of informal testing, everything seems to work the same, and performs about as well on GAE/appspot as the TG version does on our server. However, we haven&#39;t tested the code rigorously through an entire long game. The game state might get larger after many turns and could exceed the GAE request size limit. At that point, since the client/server interactions are only happening in javascript, behind the scenes, to a user, it will just appear to fail silently, which will be particularly obnoxious if it happens towards the end of a game that they&#39;ve invested quite a bit of time on. Clearly more testing is needed here to figure out if/how much game state grows as the game goes on. Second, now that the game state is actually stored in the datastore rather than in ephemeral memory, it can accumulate. And at 750k&#43; for each game, that can add up quickly and exceed GAE&#39;s limits for free apps. To deal with this we should probably write a &#34;scheduled task&#34; (essentially a cron) to clear out game states that are more than a day old. I&#39;m also not sure what kinds of limits GAE puts on memory usage on a per-request basis. Even though the state can be serialized and zipped down to under 1MB, when it is processing each turn, that has to get expanded out in memory to the full game object, which must be back in the 4-8MB range. Watching the memory usage on our servers while ReliefSim is running, it&#39;s definitely a substantial memory load. I haven&#39;t triggered any throttling or errors on GAE yet with my testing, but it wouldn&#39;t surprise me too much.]",
   "tags": ["gae", "googleappengine", "simulation", "turbogears"]
},
{
   "url": "/articles/djangocon-12-submission-django/",
   "title": "DjangoCon &#39;12 Submission - Offline and Off-Road: Django, Health and Human Rights",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[DjangoCon &#39;12 is on the East Coast this year, and we submitted a proposal to present on our recent intervention in South Africa. We hope to see you in DC! Title: Offline and Off-Road: Django, Health and Human Rights Description: For years, CCNMTL has been using Django to create interactive multimedia health interventions. We&#39;ll spotlight our latest NIMH-funded project where we deployed Django to offline netbooks at South African HIV clinics, and developed a sneakernet-based (ie USB drives) data synchronization protocol. We&#39;ll also present our FOSS CMS for authoring these ebook-like sites. Abstract: For years, CCNMTL has been using Django to create interactive multimedia health interventions as a part of our Triangle initiative. We have worked closely with the Schools of Social Work and Public health to explore the possibilities and benefits of incorporating rich, interactive, multimedia into these kinds of counseling sessions. In this talk we will spotlight Masivukeni, our latest NIMH-funded project where we deployed Django to offline netbooks at South African HIV clinics, and developed a sneakernet-based (ie USB drives) data synchronization protocol. As we iterate over projects like these, we have continued to abstract the aspects of these projects that are idiosyncratic to this domain. We&#39;ll also present our open-source lightweigth-CMS that we have created for authoring these ebook-like sites. The characteristics of these sites are serial content delivery (often with specific business rules, such as preventing access to already-seen, or not-yet-seen pages), interspersed with casual learning &#34;games&#34; (eg html5/javascript drag and drop activities). Finally, we will discuss the roadmap for this authoring tool, including the possibility of a networked, collaborative ebook authoring tool, that might export epub3 or SCORM-compliant sites.]",
   "tags": ["django", "conference"]
},
{
   "url": "/articles/the-forest-and-the-trees/",
   "title": "The Forest for the Trees",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[At CCNMTL we focus on pedagogical innovation, but we continue to work on projects that involve delivering static educational materials in traditional sequential formats. We work hard to carve out places for study in a world of instruction, but there is plenty of important knowledge that people want to acquire, and training people on skills continues to be an important component of education and often a precondition of concept formation. In many of our projects, we&#39;ve explored the boundaries of what we call - &#34;Serial Directed Learning Modules&#34;. The key properties of these projects include: Nested, hierarchical, rich content with idiosyncratic navigation and access rules Rich interactive activities (quizzes, drag/drop, planning, mapping) Detailed reporting on the learner&#39;s performance and completion In our partnership with the Columbia University Medical Center and our strategic Triangle Initiative we&#39;ve worked on several multimedia behavioral interventions that conform to this delivery pattern. We&#39;ve worked on direct interventions relating to HIV couples counseling, childhood diabetes and cavity prevention, treatment adherence, and we&#39;ve developed directing learning modules for teaching practitioners about tobacco cessation, child abuse, and more. While similar in the abstract, these projects vary in their devilish details. Some of these environments are mediated by a service provider, such as a social worker and their patient, while others are self-directed. Some require multiple modes with additional notes available only to the facilitator. A few lessons are completed in a single sitting, while others must preserve state and pick up where the learner left off. We try to balance the effort of creating unique works of art with churning out boilerplate, cookie-cutter sites. We&#39;ve explored the use of general purpose content management solutions (CMS) for these projects and are regularly stymied by the mismatch between these styles of interaction and the sweet spots of the CMS platforms we know well. CMS platforms are great for creating collections of random-access content, and organizing and relating it in a variety of ways. The business rules around the directed learning projects often left us wrestling with CMS environments, wishing we had developed them using a lightweight MVC framework, without as much overhead to introduce the customize workflows these projects demand. After building of a few of these sites à la carte, we began to generalize our approach and developed the PageTree hierarchical menu-creation system for Django. PageTree evolved into an open-source lightweight, domain-specific content management system, and we introduced a modular architecture for embedding and assembling PageBlocks which introduces elements like text, media, or custom Javascript activities within pages. The source code for PageTree and a basic set of PageBlocks are available on our &#39;ccnmtl&#39; github account. We have also released the code and content powering the childhood diabetes intervention - and it is available here. As the demand for these sites has grown, we&#39;ve recently created a system for &#34;farming&#34; these PageTree sites -- aptly named &#34;Forest&#34; -- that allows our project managers to very quickly set up their own PageTree sites (called &#34;Stands&#34;) in order to get a skeletal site up and running without the bottleneck of overhead of developer intervention. You can see a self-documenting demo of Forest here. This approach allows us to collect content as early as possible. The features can be developed around the content, instead of vice-versa. If the site requires custom functionality that goes beyond the generic features of the Forest farm, we can spin off an independent Django site from the Forest farm, and begin development at the onset with the site&#39;s content already in place. This system helped us achieve a nice balance between customization and efficiency, and we are pleased with the flexibility this approach has enabled for this class of projects. We&#39;re in the process of conceptualizing a roadmap for PageTree sites, and have been imagining a collaborative authoring platform that supports versioning, SCORM authoring/publishing platform, BasicLTI compliance, and more.]",
   "tags": ["agile", "directedlearning", "django", "pagetree", "scorm"]
},
{
   "url": "/articles/how-to-get-pcp-on-the-web/",
   "title": "How to Get PCP on the Web",
   "author": "Anders Pearson",
   "date": "",
   "content": "[For years, I&amp;#8217;ve watched our video team do amazing work shooting, editing, and encoding video for the web. I think most production companies would be shocked at how much high quality work our team produces with so few staff, a tight budget, and tighter time constraints. When I look closely at how they do what they do, I&amp;#8217;m impressed and just a little frightened at how many manual steps are involved in getting a video online. Manual steps that take time, attention to detail, expertise, and are ripe for mistakes. I try to automate everything I touch. I can&amp;#8217;t help it. It&amp;#8217;s the curse of being a programmer. I&amp;#8217;ve looked at our video process a few times over the years, always thinking about how we might eliminate some of those manual steps. Aside from my instinctive cringing at anything that looks repetitive to me, the work we&amp;#8217;re doing in online learning (and the web in general) is integrating more and more student created or supplied video. The options for that have not been the best. If there were no issues of copyright or privacy, we could just tell students to upload their videos to youtube and be done with it. Unfortunately, that&amp;#8217;s usually not possible, so we end up with students having to bring their videos to us, on flash drives or SD cards or floppy disks or punchcards or whatever they have and our video team collects them, grabs the files and processes and uploads them for them. This is a big drain on our resources and takes their time away from the shooting and editing work that actually makes better use of their skills and talents. It&amp;#8217;s also inconvenient for students to have to come to Butler Library during business hours to drop off their files, pick up their drives later, and so on. If they have to upload a video for a class assignment, they&amp;#8217;d be much happier if they could just upload it directly themselves at 2am the night before the assignment is due (because we all know that that&amp;#8217;s how it works). Finally, if every student supplied video has to come through our video team, it puts a severe limit on how many video related assignments can realistically happen at once and how large of a class can run them. When I&amp;#8217;ve looked at this stuff in the past, I&amp;#8217;ve usually run into a wall pretty early on. This video encoding stuff is hard. All the possible input formats, codecs, bitrates, and aspect ratios are a royal pain. The tools that our video team uses to deal with those are generally OS X desktop apps that expect a user to point and click and aren&amp;#8217;t that concerned with exposing an API to script. Then the delivery options as far as streaming servers, authentication schemes and podcasting tools, each with their own picky, proprietary interfaces just multiply the complexity. If all that weren&amp;#8217;t enough, integrating anything involving video with our web application world has a big, fat elephant of a problem: video is big. The files are orders of magnitude bigger than the images or text or rows of data in databases that we&amp;#8217;re used to dealing with in our web apps. The web servers we run don&amp;#8217;t have much disk space available on them (drives for those servers are much more expensive than consumer hard drives) and would fill up immediately if we put videos on them. Encoding jobs on long, high quality videos can take hours. Building web applications, I&amp;#8217;m used to worrying about whether a request is taking too many milliseconds to complete. If an application takes more than a second or two to respond, users complain. The desire to get our video work more tightly integrated with our web applications and smoothly support user supplied video content doesn&amp;#8217;t go away though. Some technologies and architectural patterns that have come out and that I hope to write about here in the future have offered solutions for the size related problems and we recently realized that enough of those impediments have eroded that it was worth investing some effort into the problem again. In the last few years, improvements have also been made in our video process thanks to a lot of work building on Apple&amp;#8217;s Podcast Producer software and it&amp;#8217;s ability to manage custom workflows. Podcast Producer (or &amp;#8220;PCP&amp;#8221; as we like to abbreviate it) integrates with most of the other video tools we use and has allowed us to largely automate the encode and upload process in many cases and can manage distributing the workload across a grid of desktop and lab machines. PCP, being from Apple, is very OS X desktop specific though, which doesn&amp;#8217;t lend itself well to integrating with our web applications running on Linux servers. Getting videos into PCP typically requires running OS X, installing an application, and running it. There is a web interface, Kino, but it&amp;#8217;s pretty locked down, unintuitive, and won&amp;#8217;t even load in a lot of browsers. However, to me, that web interface, limited as it is, was the crack in the armor of the video problem that I&amp;#8217;d been waiting for. However we were going to go about working video upload support into our web applications, we knew we weren&amp;#8217;t going to abandon all the work that had been done with PCP. The workflows that had been developed for it handled our encoding needs and were robust and debugged. What I needed was just a way to get our web applications to be able to communicate with PCP. So I got out my dissecting tools and started figuring out how to talk to Kino from Python. The result is a little library called Angeldust which we have released in case anyone else needs to do something similar. The code is relatively short but it took quite a while to get there. Kino clearly was not intended to be used in this way and fought me every step of the way. It was undocumented and used some odd HTTP headers for SSL stuff (I don&amp;#8217;t remember the exact details now, but it was why it wouldn&amp;#8217;t even load in most non-Apple browsers). It used a weird combination of HTTP Basic Auth as well as cookie based login sessions. The interface for the site was built, not as plain HTML, but as an almost completely client-side JavaScript application (similar to GMail) constructed from obfuscated and minified JavaScript. This made it a pain to figure out what form parameters were being used and when they were being passed back and forth to the backend. When submitting a video to be processed by a workflow, Kino broke it into two steps. First, you would select the workflow, then you would upload the video with its title and description. It does it in two separate requests to the backend with the state stored in the session. This was easy enough to figure out and deal with, but this kind of stateful interface is annoying and fragile since the underlying HTTP is stateless. An application trying to interface with Kino has to make two separate requests to accomplish one action: first set the workflow, then submit the video. This opens it up to race condition bugs in a concurrent environment if one isn&amp;#8217;t very careful. The Kino interface has a few more adminstrative features, but the only functions we really needed were to get a list of the PCP workflows that are available, listed by their title and UUID, and to submit a video to one of those workflows. This is the functionality that angeldust exposes. Using it is fairly straightforward: from angeldust import PCP pcp = PCP(&#34;https://mykinoserver/url/&#34;,&#34;username&#34;,&#34;password&#34;) for workflow in pcp.workflows(): print &#34;workflow &#39;%s&#39; has UUID %s&#34; % (wf[&#39;title&#39;],wf[&#39;uuid&#39;]) pcp.upload_file(open(&#34;some_video.avi&#34;,&#34;rb&#34;),&#34;some_video.avi&#34;,&#34;uuid-of-workflow&#34;,&#34;title&#34;,&#34;description&#34;) angeldust handles everything else for you. It&amp;#8217;s careful to stream the video upload in chunks instead of trying to read the entire file into memory first. Unfortunately, there are still things that angeldust can&amp;#8217;t really do and won&amp;#8217;t be able to without some changes to Kino. First, filename, title, and description are the only metadata fields available. PCP has the ability to deal with more metadata, but Kino actively ignores anything except those couple fields. Actually, we&amp;#8217;ve found that Kino also loses the original filename as soon as the video is uploaded, before it makes it into the PCP workflows. Aside from preventing us from exploring some more interesting automatic publishing use-cases, It makes it hard to even track an uploaded video through it&amp;#8217;s whole lifecycle. Filenames and titles aren&amp;#8217;t typically enough to uniquely identify a video, so we end up having to insert unique ids into those fields in ad-hoc and brittle ways. Angeldust is the first, key piece in getting PCP to integrate with web applications for student supplied video and we hope that others will find it as useful as we do.]",
   "tags": ["pcp", "python", "video"]
},
{
   "url": "/articles/deploying-legacy-turbogears-projects/",
   "title": "Python Deployment Chronicles: Deploying legacy TurboGears projects with modern tools",
   "author": "Ethan Jucovy",
   "date": "",
   "content": "[At CCNMTL most of our new Python projects are written in Django, but we still support a number of older projects that were written with TurboGears 1.0.4. They&#39;ve continued to be stable, and we don&#39;t do a ton of new development on them, so it hasn&#39;t been worthwhile to upgrade them to newer versions of TurboGears. But we do occasionally make changes to their code, and recently we&#39;ve begun migrating them to newer servers. &amp;nbsp;So I recently spent some time updating their deployment processes to CCNMTL&#39;s current best practices: Installation with pip instead of easy_installFully pinned local source distributions versioned alongside the codeNo Internet access required anywhere in the deploymentContainment with virtualenvI ended up with a package that you can use to create an isolated TurboGears 1.0.4 environment to run legacy projects in, or (if for some reason you want to) to create new TurboGears 1.0.4 projects. &amp;nbsp;You can get it on Github here:&amp;nbsp;https://github.com/ccnmtl/turbogears_pip_bootstrapper In this post I&#39;ll go into detail about what it does, and the hurdles I ran into along the way. Like our Django bootstrap process, this is a one-step installation procedure for creating an isolated TurboGears 1.0.4 environment. &amp;nbsp;Just run bootstrap.py and you&amp;rsquo;ll have a virtualenv in ./ve, with all the packages you need to run or create a TurboGears 1.0.4 project. (If you&amp;rsquo;re curious what I&amp;rsquo;m upgrading this from,&amp;nbsp;Anders has a post describing our historical TurboGears bootstrap process.) Switching from easy_install to pip From Eggs to Tarballs Until now, we were using&amp;nbsp;easy_install&amp;nbsp;to install the packages we needed. &amp;nbsp;Switching to pip is usually straightforward, but we&amp;rsquo;d been bundling all our dependencies as eggs instead of source tarballs, and pip doesn&amp;rsquo;t support installation from eggs. So the first step was collecting source tarballs for each of the eggs we were installing into a TurboGears environment. &amp;nbsp;This was mostly just a matter of downloading the .tar.gz file from PyPI that corresponded to the exact version of the package we were installing from an egg. &amp;nbsp;Some of them were hard to find or weren&amp;rsquo;t on PyPI;&amp;nbsp;http://files.turbogears.org/eggs/&amp;nbsp;and&amp;nbsp;http://peak.telecommunity.com/snapshots/&amp;nbsp;were&amp;nbsp;helpful here too, and for one or two particularly stubborn dependencies I just had to get the right version from SVN and package it in a tarball myself. To speed up the process, I wrote a script that tries to find and download all the right tarballs, given a directory of eggs. &amp;nbsp;After a few iterations, that took me most of the way. &amp;nbsp;I was able to find the rest by hand, and eventually I had all the source distributions I needed. &amp;nbsp;(You can get them all here.) The Requirements FileThe last step was writing a pip requirements file that listed all of the dependencies. &amp;nbsp;This was tricky for two reasons: The order that you list the requirements matters &amp;ndash; if package A depends on package B, you have to list A before B in your requirements file. &amp;nbsp;Otherwise pip might end up downloading a copy of B from the Internet before it even sees that you&amp;rsquo;ve specified a local source distribution. &amp;nbsp;We want to prevent deployments from requiring Internet access (and relying on PyPI uptime) &amp;ndash; and, most likely, you&amp;rsquo;ll end up with the wrong version of package B installed.We use a couple of TurboGears plugins, and those plugins try to import turbogears in their setup.py files &amp;ndash; meaning that you can&amp;rsquo;t install them until after&amp;nbsp;TurboGears itself is fully installed.For the former, it was &amp;ndash; again &amp;ndash; just a matter of time to get it right: run the bootrap script, see if pip tries to download any packages, reorder the lines in requirements.txt, and repeat until it all worked right. &amp;nbsp;To make it a little easier, I passed an &amp;ndash;index-url=&amp;rdquo;&amp;nbsp;option to pip, which made it fail early and loudly the first time it tried to download anything. &amp;nbsp;(I blogged about this trick for preventing pip network access a few weeks ago.) For the latter, reordering the requirements file wasn&amp;rsquo;t sufficient &amp;ndash; because pip runs setup.py egg_info on all packages in your requirements file before it runs setup.py install on any of them &amp;ndash; so when it got to the packages that tried to import turbogears in their setup.py, it failed with an ImportError. &amp;nbsp;So I made a second requirements file, and I ran pip install a second time to install these packages after TurboGears and its&amp;nbsp;dependencies were all fully installed. Pinning SetuptoolsThe only other tricky thing was getting the right version of setuptools installed. &amp;nbsp;The version of TurboGears we&amp;rsquo;re using relies on setuptools 0.6c8 &amp;ndash; versions later than that cause errors with static file serving and other things, because of changes in pkg_resources. Virtualenv is pretty opinionated about what version of setuptools it installs &amp;ndash;&amp;nbsp;https://github.com/pypa/virtualenv/issues/89&amp;nbsp;has the details. &amp;nbsp;For my purposes, the simplest thing to do was modify the copy of virtualenv.py I&amp;rsquo;m providing. &amp;nbsp;I made a few changes to the file so that, instead of saying &amp;ldquo;install setuptools 0.6c11 or greater,&amp;rdquo; it instead says &amp;ldquo;install setuptools 0.6c8 exactly.&amp;rdquo; &amp;nbsp; To prevent network access and ensure full version pinning, I also dropped in local copies of a setuptools 0.6c8 egg and a pip 1.0 tarball, and told virtualenv.py where to find them. The resulting bootstrap repository can be dropped into all of our TurboGears 1.0.4 projects, letting us upgrade our builds to pip, virtualenv, source distributions, no dependency on Internet access, and full version pinning in a single step. &amp;nbsp;In case anyone else has a similar need to modernize their deployment processes for a frozen TurboGears 1.0 project, we&amp;rsquo;ve made the code available on Github. &amp;nbsp;Patches welcome!]",
   "tags": ["deployment", "process", "python", "turbogears"]
},
{
   "url": "/articles/new-features-in-virtualenv/",
   "title": "Python Deployment Chronicles: New features in virtualenv",
   "author": "Ethan Jucovy",
   "date": "",
   "content": "[Earlier this week, I wrote about how to make virtualenv install pip and setuptools from local source distributions, instead of fetching unpinned copies of them from the Internet, which it does (somewhat silently) by default. The approach relied on a somewhat buried feature of virtualenv: looking for appropriate distributions in a virtualenv_support directory before downloading them. In a future release of virtualenv, this will be easier, and also more apparent. &amp;nbsp;I submitted patches for two new features which were accepted by virtualenv&#39;s maintainers: An --extra-search-dir=/path/to/directory command-line argument, which lets you put pip/setuptools/distribute distributions wherever you want on the filesystem.A --never-download flag, which will cause virtualenv.py to fail during installation if local distributions aren&#39;t found, instead of downloading packages from the Internet; useful if you want to be alerted early and loudly if your deployments have inadvertent Internet dependencies.These new features are documented in the source here. &amp;nbsp;If you want to start using them now, you can fetch a copy of virtualenv.py from the &#34;develop&#34; branch:&amp;nbsp;https://github.com/pypa/virtualenv/raw/develop/virtualenv.py]",
   "tags": ["deployment", "process", "python"]
},
{
   "url": "/articles/pinning-setuptools-and-pip-vir/",
   "title": "Python Deployment Chronicles: Pinning virtualenv, setuptools and pip",
   "author": "Ethan Jucovy",
   "date": "",
   "content": "[In my previous post I talked about how to ensure that none of your Python project&#39;s dependencies are being downloaded from the Internet when you create a fresh virtualenv and install them. This is good for deployments: each deployment is completely reproducible since every package&#39;s source is installed from a specific version of the codebase that&#39;s versioned alongside the code you&#39;re deploying, and deployments don&#39;t require external network access to succeed. There&#39;s one piece that&#39;s still missing, though: isolating and pinning the installation of the installation/bootstrapping tools themselves -- virtualenv, pip, and setuptools. Virtualenv / Pip AnatomyFirst, a quick run-through of the relationship between them all. &amp;nbsp; Pip can be made to invoke virtualenv, and invoking virtualenv will install both pip and setuptools, so it&amp;rsquo;s a bit hard to keep it all straight. When you create a new virtualenv, the virtualenv tool will pre-install pip and setuptools (or, if you prefer, a setuptools fork called distribute) in your new environment. &amp;nbsp;I&amp;rsquo;ll come back to this fact later in the post. Meanwhile, you can also create a new virtualenv with pip: $ pip -E /path/to/virtualenv install SomePackage That command will install SomePackage into the virtualenv at /path/to/virtualenv, creating the virtualenv at /path/to/virtualenv if it doesn&#39;t already exist.&amp;nbsp;&amp;nbsp;At CCNMTL, this has been the entry-point for our bootstrap script; basically we run $ rm -rf ./ve &amp;amp;&amp;amp; pip -E ./ve install -r requirements.txton every deployment to create a completely new virtualenv with the needed set of packages installed. Where do the pip and virtualenv commands come from, though? Virtualenv comes in a convenient single-file version which you can run with the python command of your choice. Of course you can install it too (using easy_install, pip, your OS packaging system, etc...) but I find it&#39;s easier to keep track of if you just grab the file and invoke it with python: $ wget https://github.com/pypa/virtualenv/raw/master/virtualenv.py $ python virtualenv.py --help One advantage here is that you can invoke this file with any version of Python, rather than needing to install a separate version for each Python on your system. &amp;nbsp;Another is that you can check that file into your project&#39;s code repository, isolate it from the rest of the system, and version it along with your code. &amp;nbsp;In other words, all those advantages of per-project containment. The same used to be true of pip -- it came in a single file, pip.py, which you could just grab a copy of, check in to your source repository, and run in your bootstrap script. &amp;nbsp;So this is how we&#39;ve been doing our deployments at CCNMTL -- each project has its own copy of that pip.py file, and uses it to create a virtualenv and install our project&#39;s dependencies. Around version 0.7 pip stopped being a single file, and became a package instead. &amp;nbsp;(If you&#39;re really interested, that refactoring was discussed&amp;nbsp;here&amp;nbsp;and implemented&amp;nbsp;starting here.) &amp;nbsp;So, now, you really have to install it somewhere. &amp;nbsp;Of course, as I mentioned above, you&#39;ll get a version of pip conveniently installed for you in each new virtualenv you create, so usually you don&#39;t even notice this fact. At CCNMTL we&#39;ve avoided this problem so far: all of our projects are still using the old single-file version of pip. &amp;nbsp;Since it&#39;s contained and versioned with each project, it&#39;s easy not to upgrade, and so far we haven&#39;t run into any show-stopping bugs or missing features that would force us to upgrade. There are a few messy details though. &amp;nbsp;Unlike virtualenv, pip isn&#39;t entirely self-contained -- it relies on a few other non-stdlib Python modules being importable when it runs, including virtualenv, setuptools and pkg_resources (which comes with Setuptools). &amp;nbsp;At CCNMTL, we&#39;ve handled that in various ways -- we check in a virtualenv.py file with the source next to the pip.py file, and we usually have setuptools and pkg_resources installed system-wide, even though we rarely use these system-wide installations. But, because it&#39;s no longer distributed as a single file, and because it requires precisely the modules that Virtualenv provides for you in a fresh environment, it makes more sense to just start deployments from a local, single-file virtualenv.py, rather than from pip: $ rm -rf ./ve &amp;amp;&amp;amp; python virtualenv.py ./ve $ ./ve/bin/pip install -r requirements.txt This way you get everything you need, isolated in your virtualenv, without even needing setuptools installed globally. &amp;nbsp;(Indeed, since I started writing this post, this recommendation was added to the pip docs.) Pinning setuptools and pipSo, you&#39;ve checked in a copy of virtualenv.py in your source distribution, and your deployments start by invoking it. &amp;nbsp;So far, so good -- virtualenv.py is now effectively pinned to a known-good version (whichever version you downloaded it at) and, since you&#39;ve just checked in the file with your project&#39;s source, it doesn&#39;t require Internet access and is properly versioned alongside your project and all its other dependencies. But, at this point, you may be wondering how Virtualenv gets setuptools and pip installed if you aren&#39;t relying on any system-level packages. The answer? &amp;nbsp;It downloads them from the Internet. So now we&#39;re sort of back where we started -- everything about the bootstrapping process is completely version-pinned, isolated from network dependencies, and self-contained, except for setuptools and pip! &amp;nbsp;They&#39;ll still be fetched from PyPI (better hope PyPI&#39;s up, and you have network access, during your deployment) and they&#39;ll end up installed at their latest released versions (better hope your deployment wasn&#39;t relying on some now-unsupported edge-case behavior). Luckily, virtualenv has a somewhat hidden feature to get around this: before going to the network to install setuptools and pip, it will look for local distributions of each of them in a couple of places, including a directory named virtualenv_support&amp;nbsp;in the same parent directory as your virtualenv.py file. &amp;nbsp; This gives us an easy way to pin-and-localize setuptools and pip as well. &amp;nbsp;Grab an appropriate Setuptools .egg (yes, it has to be an .egg, not a tarball) and a pip tarball from their PyPI pages, drop them in a new virtualenv_support directory, and check that in to your project&#39;s source repository next to the virtualenv.py file. &amp;nbsp;If for some reason you need an older version of Setuptools or pip, no problem -- just fetch the version you need instead of the latest release. Now, finally, your entire project deployment, with all of its dependencies, is versioned alongside your code, pinned to specific versions, and completely isolated from external network access -- including the dependencies of the deployment bootstrapper itself. Here&#39;s an example of what this might look like in a project. &amp;nbsp;The virtualenv_support directory contains local copies of setuptools and pip; a copy of virtualenv.py is located in the same directory as virtualenv_support; and a simple bootstrap script creates a fresh virtualenv (implicitly using those local versions of setuptools and pip) and then uses the new virtualenv&#39;s pip and easy_install scripts to install the project&#39;s requirements, also from local source distributions.]",
   "tags": ["deployment", "django", "process", "python"]
},
{
   "url": "/articles/the-scope-of-annotations-and-the-svg-monster/",
   "title": "The Scope of Annotations and the SVG Monster",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[I just came back from the&amp;nbsp;Open Annotation Collaboration (OAC) workshop, working to develop a standard format for annotations on images, video, text and other resources.The group&#39;s current work is on the right track--there are two parts to any annotation: the &#39;constraint&#39; which is the part of a resource we&#39;re focusing on, and the &#39;body&#39; which is the metadata describing details of that constrained section of the resource. A &#39;constraint&#39; is mostly a simple section of an image like on the right where I circle the embedded rotating calendar in a ~17th century book on Hebrew Calendars, but the general idea of a scholarly annotation can be very complex. Just how crazy? Well, perhaps I want to show a certain rotation of the discs... The OAC is planning to use a graphic standard called&amp;nbsp;Scalable Vector Graphics (SVG) to represent complex sections of an image. The problem is SVG is an incredibly complex specification that can do all kinds of manipulation. I&amp;rsquo;ve made an SVG document which references the Hebrew manuscript page, but also&amp;nbsp;clips out each disc and rotates it a certain amount. Furthermore, it includes Javascript, just like HTML which lets you rotate the discs just like you can with the original manuscript: SVG Calendar rotated(requires Firefox 4 or IE9 &amp;ndash; Safari/Chrome have a bug so try this one) This transformation on the image is a legitimate target of annotation. An academic might want to show the discs aligned in a certain way to make a point. To SVG&amp;rsquo;s credit, we can do just that, and more &amp;ndash; we can make it possible to rotate the discs differently and refer back to how much it is rotated. When script tags are included (which SVG allows), each annotation can be a complete computer program. While this is a powerful tool, the reason we are interested in a standard for annotations is so that we can interchange them across systems. That means each system needs to be able to understand annotations from other systems.&amp;nbsp; While the&amp;nbsp;Media Fragments W3C group&amp;nbsp;is helping to define annotations with a conservative scope &amp;ndash; box areas in images and timecodes in video, the OAC ambitiously is going after a much wider understanding of annotations. We&amp;rsquo;d like our multimedia annotation platform,&amp;nbsp;MediaThread&amp;nbsp;(which already supports mediafragments) to support the OAC both for importing annotations from elsewhere and for exporting the annotations made by users in the system. This should make us able to consume more media across the web, and hopefully, as the OAC is adopted across platforms, provide a useful file format for multimedia compositions. MediaThread is unlikely to support the calendar annotation anytime soon. For images, we allow &amp;lsquo;laser-pointer&amp;rsquo;&amp;nbsp;annotations which are about highlighting some subset from a resource, and probably covers the vast majority of reasonable annotations for scholarly reference.&amp;nbsp; These should be simple to make and simple to parse, so I think we&amp;rsquo;ll need to standardize the very small subset of the SVG specification for these kinds of annotations.&amp;nbsp; I&amp;rsquo;m looking forward to that and other challenges with the OAC. An annotation standard will create huge opportunities for tools to engage archives and for academic publishing.&amp;nbsp;]",
   "tags": ["annotations", "digital humanities", "mediathread", "svg"]
},
{
   "url": "/articles/preventing-network-access-with/",
   "title": "Python Deployment Chronicles: Preventing network access with &#34;pip install&#34;",
   "author": "Ethan Jucovy",
   "date": "",
   "content": "[Anders has written several times about our deployment strategy for Django apps at CCNMTL. Aside from containment of each project with virtualenv, we also try to make sure that deployments never depend on anything external, and can be done without access to the wider Internet. We do this by an aggressive form of version pinning: in each project&#39;s repository, we check in source tarballs of all the project&#39;s dependencies, including Django itself. We then have a pip requirements file that points to each of these local files in order. (Here&#39;s an example, and the bootstrap script that uses it.) There are two benefits to this approach. First, it removes our deployments&#39; dependencies on external web services, like PyPI, being online. Second, it ensures that we know exactly what versions we&#39;re using of all the Python code in a project&#39;s deployment. That makes deployments trivially repeatable, and gives us the ability to roll back a deployment to any earlier version -- so if a new deployment doesn&#39;t work properly for some reason, we can re-deploy the last tagged deployment and know that (barring system-level changes) it&#39;ll work exactly as expected. The other week, we made a new deployment to one of our Django projects, and the site stopped working. It turned out that the wrong version of Django was installed somehow: the project was built on Django 1.0, but this broken deployment ended up with Django 1.2 instead. And, oddly, rolling back to the previous deployment didn&#39;t fix the problem. This looked to me like an unpinned nested-dependency problem &amp;ndash; where, even though we&amp;rsquo;ve pinned down specific versions of all of our dependencies, one of those dependencies itself has a dependency which we haven&amp;rsquo;t pinned down. If that nested dependency puts out a new release, our bootstrap script will start pulling in that new version, which might break our project in both old and new deployments. So I ran our bootstrap script, and searched its output for any packages being downloaded from the internet: ./bootstrap.py | grep -i downloadSure enough, two packages were being fetched from PyPI. And, as I suspected, a new release of one of those packages had just been made on the same day that our deployment broke. So, I fetched tarballs of both packages from PyPI (using the previous version of the one that just had a new release) and added them to the project&amp;rsquo;s requirements. That solved the problem &amp;ndash; the project&amp;rsquo;s deployment worked again. I also wanted to make sure this would never happen again. After all, it turned out that this project&amp;rsquo;s deployment wasn&amp;rsquo;t as contained as we had thought &amp;ndash; its pinned dependencies had unpinned dependencies, meaning deployments weren&amp;rsquo;t 100% repeatable, and we needed access to PyPI for the build to succeed. Luckily pip has a flag that lets you set a&amp;nbsp;URL&amp;nbsp;other than http://pypi.python.org to fetch packages from. Setting this to a bogus&amp;nbsp;URL&amp;nbsp;was a simple way to ensure that installation would fail early and loudly, with a non-zero exit code, if we have any dependencies that aren&amp;rsquo;t provided as source tarballs in the project checkout: pip install PackageName &amp;ndash;index-url=&amp;rdquo;With that option set, instead of fetching unpinned packages from the Internet, pip will fail with a message like&amp;nbsp;ValueError: unknown url type: &amp;ldquo;/PackageName. Here&amp;rsquo;s&amp;nbsp;the actual change in our bootstrap script. &amp;nbsp;With this change, our projects&amp;rsquo; deployments are fully self-contained and repeatable, and also self-guarding against programmer error: if we gain any new dependency and forget to provide local pinned source distributions of all of its dependencies, the build will fail immediately and noticeably, rather than potentially getting into production with unvetted versions of its dependencies pulled at deployment-time from the Internet. (As for why the new release of this package resulted in an installation of Django 1.2 even though we told pip to install a local Django-1.0 source distribution, it turns out to be an apparent bug in pip involving a very specific combination of local source distributions, network download of unpinned requirements, and case-sensitive nested dependencies. I haven&amp;rsquo;t managed to come up with a patch for the underlying issue, but I reported&amp;nbsp;steps to reproduce the bug in pip.)]",
   "tags": ["deployment", "django", "process", "python"]
},
{
   "url": "/articles/now-touch-me-baby/",
   "title": "Now touch me, baby",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[Walking past the round-the-block lines for the iPad 2 prompted me to reflect on the long-term educational significance of the tablet. The futurists who predicted the rise of mobile will most likely claim that these tablets fulfill their vision. Tech pundits insist that this new form-factor goes beyond superficial cosmetics and represents a transformation in computing. What should we make of the hype? The 2011 Horizon Report predicts both mobile and ebook/ereader adoption within the next year: &#34;For more involved web browsing, reading, watching videos, or to use any of the tens of thousands of Internet productivity and lifestyle applications, the tablet provides just enough extra space to enable comfortable use over longer periods of time.&#34; At CCNMTL we&#39;ve begun exploring the deployment of applications on tablets. We recently launched two behavioral interventions that are being used in the field - MySmileBuddy and MedER. These applications are hybrids of data collection and multimedia educational material and are being administered by social workers making housecalls and researchers on-site in the pediatric emergency room. These applications are designed to be used by two or three people in natural settings, without workstations. The tablet is poised to take the place of the clipboard in a variety of interactive contexts. A smartphone screen is too small for this usage, and a laptop is too awkward for this use. Other educational technology groups have also been exploring the purposeful use of tablets in teaching, learning, and research. Duke University has been experimenting with the iPad as a fieldwork research tool. Their efforts have focused on assembling a useful suite of existing applications, rather than developing any custom applications. Our early projects with touchscreen interfaces have taught us how different the user experiences are on these devices. Clicking, dragging, scrolling, and paging all need to be rethought for a quality tablet experience. Our technical strategy for developing these applications has been to follow a standards-based, HTML5/JavaScript/CSS approach. This approach matches our existing culture, workflows, and processes, and is compatible with our long-term belief in the web browser and general purpose computing. 37signals, the patron saints of the Ruby on Rails framework, is taking a similar approach with its new Basecamp Mobile application (the underlying mobile framework will soon be released under an open-source license). In upcoming posts we will share some of the lessons and implementation details of an HTML5 multimedia iPad application that can be used both on and off line. The spin machines have kicked into overdrive around these new devices, but the excitement is not all hype, as mobile tablet computing does offer a wide range of innovative approaches to content delivery, interactive mobile applications, and data collection.]",
   "tags": ["ipad", "tablet"]
},
{
   "url": "/articles/deploying-django-and-deploying-with-django/",
   "title": "Deploying Django and Deploying With Django",
   "author": "Anders Pearson",
   "date": "",
   "content": "[The other night I was on a panel discussion on Python Deployment at the Django-NYC meetup. The discussion was very good and I think everyone there got a lot out of it (I did). But the format wasn&amp;#8217;t really suitable for going into great detail, showing actual code fragments or demos. I&amp;#8217;ve written about aspects of our deployment strategy in the past on my personal blog and in comments on other sites, but it&amp;#8217;s a continuing work in progress and I&amp;#8217;m well overdue for an update. We also have a web-base deployment tool that, while the code has been open and available for a while, we really haven&amp;#8217;t officially announced or publicized until now. There are two angles to Python/Django deployment that I want to discuss. First, there&amp;#8217;s deploying Django apps. Then there&amp;#8217;s how we use a Django app to deploy Django (and other) apps. They are closely intertwined so I think I can&amp;#8217;t really talk about one without talking about the other. A bit of background. CCNMTL has been around for over a decade, building custom educational software for the Columbia community and beyond. We have six programmers, a couple designers, a dedicated video team, numerous educational technologists, and assorted others. Our projects are of varying size and complexity, but there&amp;#8217;s been a fairly steady pace of launches every few months for most of our history. Our technology stack is heterogeneous and has varied over time as well. We currently deploy a number of web applications built on Python (with Django, TurboGears and Plone), Perl, PHP (Drupal, WordPress), and Java. While we may launch new apps every few months, the old ones don&amp;#8217;t go away at that rate. Some of the applications we currently maintain have been running for the better part of the last decade in one form or another. Deploying Django For deployment in general, we apply Spolsky&amp;#8217;s recommendation of a one step build. Deploying an application to production needs to check out the current version of the code from version control, sort out any dependencies, run any build steps needed, run unit tests, get the code onto the production server, and restart or HUP whatever server processes there need poking. If the deployment procedure requires any more manual steps than clicking one button, someone&amp;#8217;s bound to mess it up and trouble will not be far behind. Since we have multiple developers and occasionally have to do a quick emergency fix on someone else&amp;#8217;s app, it&amp;#8217;s extra important that the deployment process be as simple and foolproof as possible. Furthermore, the deployment scripts act as necessarily accurate and up to date documentation on our server setup. With lots of separate codebases each having potentially very long lifespans, our approach to deployment has skewed very much towards maintaining isolation between the environments. If we need a newer version of a Python library for a new application we&amp;#8217;re launching, we simply don&amp;#8217;t have the manpower to go through the other 20 applications running on the same server and test that they are going to be compatible with the new version of the library. As much as we can possibly manage, we need to keep them separate and isolated. We achieve this isolation on a few levels. First, we make heavy use of server virtualization to isolate on a very rough level. We currently segregate roughly based on technology. We have a virtual server for Django apps, one for Plone apps, one for TurboGears apps, one for Perl, etc. This simplifies administration greatly and allows us to tune the performance of the machines differently for different kinds of workloads. We also run several development and staging servers. What&amp;#8217;s more relevant here though is the isolation between Django apps on the same server. Our Django virtual machine really does currently run about twenty separate applications (I wasn&amp;#8217;t exaggerating before). They range from Django 0.96 to Django 1.2. Most of our new applications these days are developed on Django, but we started working in earnest on the isolation side of deployment back when we were doing a lot of TurboGears. The heavy lifting in this setup is done by virtualenv, which will probably come as no surprise. Mostly what we&amp;#8217;ve done is put a nice, cozy layer of conventions on top of virtualenv to make it even more streamlined for our situation. Another aspect of deployment that we&amp;#8217;ve come to a strong opinion about is that deploying to production should not ever depend on anything external. This is a hard won lesson from early TurboGears days. Back when we were using it heavily, TG was a bit of a beast as far as dependencies. It was pretty much the poster child for setuptools and eggs because it had a lot of dependencies. Installing TurboGears or updating it involved setuptools downloading eggs from a dozen different sites around the web. Of course, that practically guaranteed that at any given time, one of those sites would be down, so your install or upgrade would fail. They quickly consolidated and mirrored all the eggs on turbogears.org which improved the situation until tg.org started becoming unstable. Nowadays, I think their requirements are all on PyPI, which is probably an improvement again, but still a potential problem if your deployment hinges on having to download a library. Since PyPI is a common single point of failure for deployments, there&amp;#8217;s ample discussion in the Python community about different techniques for making local mirrors of PyPI. Our approach is simpler than mirroring PyPI though perhaps a bit more drastic. We just check in a copy of every library that the application needs along with it in a &amp;#8216;requirements&amp;#8217; directory and use a bootstrap.py script that installs all of those requirements (and only those requirements) into a per-application virtualenv. Our bootstrap.py script looks like this: #!/usr/bin/env python import os import subprocess import shutil pwd = os.path.dirname(__file__) vedir = os.path.join(pwd,&#34;ve&#34;) if os.path.exists(vedir): shutil.rmtree(vedir) subprocess.call([&#34;python&#34;,os.path.join(pwd,&#34;pip.py&#34;), &#34;install&#34;, &#34;-E&#34;,os.path.join(pwd,&#34;ve&#34;), &#34;--requirement&#34;, os.path.join(pwd,&#34;requirements/apps.txt&#34;)]) It&amp;#8217;s brutal but effective. First, it does an &amp;#8220;rm -rf ve&amp;#8221; to clear out any old virtualenv directories so we can make sure we&amp;#8217;re starting fresh. Then it does python pip.py install -E --requirement requirements/apps.txt Which creates the new virtualenv in ve and installs the libraries listed in requirements/apps.txt, which is a just a text file listing one .tar.gz per line corresponding to the tarballs in the requirements/ directory. Those libraries in requirements/ are everything. Even the Django tarball goes in there. It seems a bit wasteful to duplicate all of those tarballs on every application we run, but it&amp;#8217;s a trade-off we&amp;#8217;re happy to make. Ultimately, each requirements directory comes in around 20 to 30 MB, which is pretty small in the scheme of things and git has no trouble handling them. [This is basically what pip now does with its freeze command. We had our system working before pip added freeze though and it works for us so we haven&amp;#8217;t gotten around to switching to pip --freeze yet, but we will soon.] That explains the bulk of how we keep our deployments isolated and not dependent on any external sites like PyPI. In practice though, there are a number of small details that make it actually come together nicely. We use a custom CCNMTL-specific Paste template to start new Django projects and it plays a big role. Python Paste Templates fill a role similar to django-admin.py startproject. They will create a directory structure for you and pre-populate it with a number of files, some of which have been generated from templates with project specific bits filled in. The advantage is that Paste Templates are pretty easy to create and customize for your situation. So we have a Paste template called ccnmtldjango which is up on github and you can check it out. A lot of the customizations we use it for are related to how we bundle dependencies for our isolated deployments. When you run paster create --template=ccnmtldjango, and tell it your project name, it sets up the directory just like django-admin.py startproject would, but it also includes that bootstrap script, a local copy of pip.py, a requirements directory loaded with tarballs for Django and all the libraries that we typically use, as well as sample apache config files that are set up with all the correct paths and configurations so the application can be deployed directly to our production server (Apache and mod_wsgi) with virtualenv isolation fully in effect. Another easy to miss customization it does is to change the first line of manage.py to #!ve/bin/python. Since most commandline interaction with django involves ./manage.py some command (runserver, shell, syncdb, etc), this saves us from pretty much ever having to &amp;#8220;activate&amp;#8221; the virtualenv. The whole process of a developer starting a new Django project is pretty simple. They need to start on a machine with virtualenv, python-paste, and ccnmtldjango installed. Then they run the paster create --template=ccnmtldjango, go into the new project directory, check it into version control, run ./bootstrap.py, and from then on, ./manage.py ... works and is isolated from anything else that might be on the system. When they later push it to production, the dependencies are all right there and the deployment scripts can run ./bootstrap.py on the production server and they are guaranteed to have the exact same versions of every library in production that they had in development. Deploying with Django The second half of this topic for us is the application that we use to deploy our Django apps, as well as our TG, Plone, Perl, PHP, and Java apps. This application is Rolf. Rolf is our web-based deployment system. It happens to be written in Django, though previous incarnations were TurboGears and CherryPy based. Rolf is available on github. Rolf and its predecessors are our attempt to unify all of our deployment in one place with one reasonably nice interface. Rolf organizes our projects into individual deployments, which each have a number of stages. E.g., the typical deployment process for a Django app is to clear out the local checkout directory, pull a copy of the app out of our git repo, tag it (so we can roll back to that point in the future easily), rsync the code to the production server, run the ./bootstrap.py step (explained above) on the production server to get all the libraries installed in a virtualenv, and then touch a .wsgi file (which causes Apache to reload the code for that application). Rolf lets a user kick off that process through a nice, AJAXy web interface. As each stage runs, it logs everything that happens on STDOUT and STDERR and will halt the push if any of the stages fail. All the pushes are logged and saved, which comes in handy for forensic analysis. If a push is successful, it saves the tag for that push and allows a user to roll back to that particular tag easily should a future push break. Each stage can be written in Bash or Python. They are typically one or two command &amp;#8220;recipes&amp;#8221; using environment variables to stay generic so they can be used across multiple deployments. Rolf also includes a &amp;#8220;cookbook&amp;#8221; area to collect the useful, re-usable recipes. Rolf has fairly basic permissions, but it&amp;#8217;s enough to support a couple use-cases that we like. Our install of Rolf sits behind Columbia&amp;#8217;s central authentication service, which also provides us with the list of unix groups of each user, which we map to Django auth groups. We can then assign group permissions for each deployment. A group can have view, push, or edit permissions for a deployment. The developers and admins will get edit permissions so they can change the settings and edit the recipes for the stages of the deployment. Designers who often need to push an app out to production but shouldn&amp;#8217;t have to worry about the settings or stages are granted push permissions. Project managers probably only need view permissions so they can see if bug fixes or features they&amp;#8217;ve been monitoring have been pushed to production yet. Meanwhile, anyone not affiliated with the project can be locked out. This whole combination of tools gives us a very powerful, flexible system for us to deploy a large number of independent applications with minimal pain and suffering for our developers. This lets us focus less on the nuts and bolts of deployment on a daily basis and concentrate instead on developing unique educational tools.]",
   "tags": ["deployment", "django", "python"]
},
{
   "url": "/articles/clone-wars-jquery-ui-draggable/",
   "title": "Clone wars: jQuery UI draggables and overflow:hidden",
   "author": "Kathryn Hagan",
   "date": "",
   "content": "[While working on a Javascript interactive for the diaBeaters project, we stumbled across an interesting problem with jQuery UI draggables. To wit: if you have draggable items inside a div with overflow:hidden, they&#39;re stuck. You can&#39;t drag them out of the container -- the div just scrolls out to infinity. (Try it sometime, it&#39;s awful.) Here&#39;s the original drag-and-drop setup. The game involves dragging magnets from a menu on the left-hand side onto a refrigerator on the right. jQuery(&#34;.magnet&#34;).draggable({ revert: &#39;invalid&#39; }); jQuery(&#34;#fridge&#34;).droppable({ drop: function(event, ui) { jQuery(this).addClass(&#39;dropped&#39;); }); After some Googling, I found that others have run into this problem as well. The only solution seems to be to use helper:clone for the draggable. For good measure, I also added the appendTo and scroll options. The updated draggable setup: jQuery(&#34;.magnet&#34;).draggable({ revert: &#39;invalid&#39; ,appendTo: &#39;#fridge&#39; ,scroll: false ,helper: &#39;clone&#39; }); Cloning the draggables did, in fact, allow them to escape from the containing div, but instead of staying where they were dropped, they&#39;d just vanish. It turns out that cloning loses you a few things you&#39;d otherwise get automatically; specifically, if you want to keep your clone, you have to append it to the DOM and position it manually. Otherwise it gets deleted as soon as you drop it. So, I had to update the droppable too: jQuery(&#34;#fridge&#34;).droppable({ drop: function(event, ui) { jQuery(this).addClass(&#39;dropped&#39;); var clone = ui.draggable; clone.appendTo(this); // this assumes the mouse grabbed in the middle of the div // (so now we need .cursorAt on the draggable) var width = clone.width(); var height = clone.height(); clone.offset({&#39;top&#39;:event.pageY-height/2 , &#39;left&#39;:event.pageX-width/2 }) } }); This did fix the positioning, but the dragged items were still not behaving as I expected. Sometimes, moving one of them would inexplicably affect the positioning of another (I think this was due to them getting re-ordered in the DOM tree). In addition, the &#34;clone&#34; paradigm didn&#39;t really work with the metaphor we were trying to get across; the &#34;ghosts&#34; left behind by cloning undermined the experience of picking up and moving a single object. After fighting with this for a little while, I realized that the best solution would just be to turn cloning off once the items had been dragged out of the container div, making them single objects once more, and saving me having to deal with positioning them manually. So I created a new class for the divs within the left menu, and set only those to use helper:clone. While I was at it, I added .cursorAt so the magnets would drop consistently (since I was positioning them from the center). jQuery(&#34;.magnet&#34;).draggable({ revert: &#39;invalid&#39; ,appendTo: &#39;#fridge&#39; ,scroll: false // no more helper:clone }); jQuery(&#34;.magnet-in-menu&#34;).each( function(elem) { jQuery(this).draggable(&#34;option&#34;, &#34;helper&#34;, &#39;clone&#39;); // can&#39;t use .width() and .height() before images load var width = parseInt(jQuery(this).css(&#39;width&#39;)); var height = parseInt(jQuery(this).css(&#39;height&#39;)); jQuery(this).draggable(&#34;option&#34;, &#34;cursorAt&#34;, {&#39;top&#39;: height/2, &#39;left&#39;: width/2}); }); And modified the droppable to manage the conversion: jQuery(&#34;#fridge&#34;).droppable({ drop: function(event, ui) { jQuery(this).addClass(&#39;dropped&#39;); // if we&#39;ve got a clone, // we need to actually save it and put it where it belongs if( ui.draggable.hasClass(&#39;magnet-in-menu&#39;) ) { var clone = ui.draggable; clone.appendTo(this); var width = clone.width(); var height = clone.height(); clone.offset({&#39;top&#39;:event.pageY-height/2 , &#39;left&#39;:event.pageX-width/2 }) clone.removeClass(&#39;magnet-in-menu&#39;); // remove &#34;clone&#34; helper &#39;cause it causes more trouble than it&#39;s worth clone.draggable(&#34;option&#34;, &#34;helper&#34;, &#39;original&#39;); } } }); And now, everything works as intended. Finally.]",
   "tags": ["javascript", "jquery"]
},
{
   "url": "/articles/happy-comment-trails/",
   "title": "Happy Comment Trails",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[This semester we upgraded our WordPress Multi-User (MU) installation to WordPress3. WordPress runs our EdBlogs course blogging platform, a system we support that is designed for multi-user course blogs. WordPress3 brings the WordPress MU fork back into the fold of the core WordPress distribution and continues the gradual improvement of its technical architecture and design. We concentrated on revamping our standard themes and worked harder than I anticipated to make sure that the default experience within a newly created blog makes educational sense. WordPress is very customizable, but many of its plugins are designed for a single blog installation. They present a bewildering array of configuration options and don&#39;t always make sense out of the box. WordPress provides hooks into every nook and cranny of the system, but finding the right one often involves a coding expedition. Running an industrial WordPress farm is much more difficult than setting up a single, successful blog. When we launched EdBlogs last year, many faculty inquired about better assessment tools to help them evaluate student participation. Wordpress has some built-in facilities to list users&#39; posts, but WordPress does not expose an interface for aggregating all of their comments. All the data exists in the database but was not accessible to users. Faculty and students alike will benefit from more easily tracking each others&#39; contributions. This fall we launched a new WordPress widget that lists all of a blog&#39;s participants, the number of posts and comments that they contributed and links to a new custom profile page that displays a list of all of their posts and comments. Participants Widget: This widget is available on github if you want to take it for a spin.]",
   "tags": ["opensource", "plugin", "wordpress"]
},
{
   "url": "/articles/simplify-simplify-simplify/",
   "title": "Simplify, Simplify, Simplify",
   "author": "Susan Dreher",
   "date": "",
   "content": "[I see programmers as inherently helpful people. Given a 57-step flowchart describing the steps some poor soul has to execute manually, most programmers get a little gleam in their eye and set about providing a streamlined solution. Programmers truly love removing those inefficiencies. Meanwhile, the customer stops wrestling with a frustrating system and gets on with his job. I recently had the opportunity to untangle a complicated little process knot. The technical details are applicable solely to the Columbia community, but I think the story is a reminder that a core engineering duty is to tackle real world inefficiencies. The Story Briana, our Communication and Marketing Manager, is responsible for posting events on our CCNMTL website and getting those events into the Columbia calendar. To post the events to the Columbia calendar, she used the university&#39;s Sundial event management software. To post events to the CCNMTL website, she used our site content management software and recreated the events, adding links to the Sundial registration system. Now, Briana has enough to do without managing events in two quite disparate systems. Maurice suggested I leverage the Sundial api to pull events into our CCNMTL site. Briana could create events in a single location and eliminate the redundant efforts. The Base Technologies Sundial is a powerful system, handling basic event creation and display plus advanced registration flow including payment, waitlists, cancellations and public and university-only enrollment. Event display can be customized through an online templating interface. As mentioned, Sundial offers a well-developed api to enable custom event retrieval. Our CCNMTL site relies on MovableType to manage content. The Architecture My main goal was to (obviously) streamline event creation, while still giving Briana a great deal of control over the look/feel of the site. As Movable Type is purely a publishing system, the best approach was to use Ajax to retrieve a list of events given date and event type parameters. The Sundial api would return formatted html based on the branded templates. Briana could edit the templates directly to tweak event display. Using Ajax meant I was faced with a cross-domain scripting problem. Ajax allows requests only to the page&#39;s domain, e.g. foo.com can make calls to foo.com but not to sundialapi.com. Luckily, JSONP now provides a workaround. jQuery integrates this solution into its getJSON call. Sundial does not offer a JSON return format, so I also needed a simple.php script to provide a JSON wrapper for the Sundial server response. Here&#39;s all the pieces: eventList.html, eventDetail.html - onload() call the Ajax event retrieval interface and slot resulting html into the appropriate div. sundial.js - Ajax script to retrieve events from events.php based on page parameters. function loadEvents() { var url = &#34;jsoncallback=?&#39;; var allParams = getUrlVars(); var queryParams = {}; queryParams[&#39;start&#39;] = allParams[&#39;start&#39;]; queryParams[&#39;end&#39;] = allParams[&#39;end&#39;]; queryParams[&#39;type&#39;] = allParams[&#39;type&#39;]; queryParams[&#39;viewType&#39;] = &#39;list&#39;; queryParams[&#39;keyword&#39;] = allParams[&#39;keyword&#39;]; queryParams[&#39;eventFilter&#39;] = allParams[&#39;eventFilter&#39;]; $.getJSON(url, queryParams, function(data, textStatus) { if (data.events) { $(&#34;#eventList&#34;).html(unescape(data.events)); } else $(&#34;#eventList&#34;).html(&#34;There are presently no events to display. Please check again soon. &#34;); var title = allParams[&#39;title&#39;] ? unescape(allParams[&#39;title&#39;]) : &#34;Upcoming Events&#34;; $(&#34;#eventListTitle&#34;).html(title); }); } events.php - shim .php script to retrieve events from the Sundial server and return a JSON response to MovableType. My code is almost a duplicate of the sample.php code provided by Sundial. &#34;; $criteria = array(&#34;dateStart&#34; = $_GET[&#39;start&#39;], &#34;dateEnd&#34; = $_GET[&#39;end&#39;], &#34;maxEvents&#34; = &#34;30&#34;, &#34;viewType&#34; =$_GET[&#39;viewType&#39;], &#34;sponsor&#34; = &#34;ccnmtl&#34;, &#34;typeFilter&#34; = $_GET[&#39;type&#39;], &#34;eventFilter&#34; = $_GET[&#39;eventFilter&#39;], &#34;brand&#34; = &#34;&#34;, &#34;keyword&#34; = $_GET[&#39;keyword&#39;], ); if (strlen($criteria[&#34;dateEnd&#34;]) $value) { $url .= &#34;&amp;&#34; . $key . &#34;=&#34; . urlencode($value); } $result = file_get_contents($url); if (stripos($result, &#34;No Records&#34;) 0) { $result = &#34;&#34;; } $arr = array (&#39;events&#39;= mb_convert_encoding($result, &#39;utf-8&#39;)); echo $_GET[&#39;jsoncallback&#39;] . &#39;(&#39; . json_encode($arr) . &#39;);&#39;; ? Customized Templates - Sundial interface. Requires authorization from the Sundial folks. I overrode the Event Listing and Event Detail to control the event presentation. While I was at it, I overrode the Child Header, Child Footer, No Records, , Registration Status, Registration Form and Registration Cancel pages to add CCNMTL-specific branding. Limitations I&#39;ve run into a few limitations with Sundial. A minor pain point has been dynamically categorizing events. Sundial provides an eventType filter with a predefined list of events. We&#39;ve used eventType to slot our events into high-level categories like &#34;Academic: Conference.&#34; Sundial also provides a keyword filter -- a single word, no phrases -- that searches the name, summary and description fields, which is great for a search interface. However, Briana requires something akin to a subEventType, so workshops can be categorized into things like Courseworks or Wikispaces. Ideally, these subEventTypes would be dynamic, so I could create and query them without any help from Sundial. Short-term hack, I&#39;m using the keyword feature as a workaround but I&#39;m worried about picking up events that just happen to have &#34;Courseworks&#34; in their description. Another minor problem is the event titles. On the Columbia calendar, we want a very descriptive title for some events, e.g. CCNMTL Faculty and Instructor Workshop. On the CCNMTL site, we can drop this prefix as the CCNMTL affilitation and Workshop type is already clear. Maybe additional event attributes such as eventHeader could be added to resolve this issue. Finally... I was casting about for a quote to bring this all full circle, and found a great one from Dijkstra: &#34;The lurking suspicion that something could be simplified is the world&#39;s richest source of rewarding challenges.&#34; If you find yourself executing some mind-numbing task over and over and over, give your local programmer a call today.]",
   "tags": []
},
{
   "url": "/articles/a-survey-of-the-clouds/",
   "title": "A Survey of the Clouds",
   "author": "Anders Pearson",
   "date": "",
   "content": "[Nothing like having to spend time in a noisy, cold, cramped server room doing upgrades on hardware to make you appreciate the virtues of the cloud. Here at CCNMTL, we have a pretty diversified setup when it comes to servers and hosting. CUIT provides a lot of resources for us: a very solid network infrastructure, DNS, email, file serving, databases, course management systems, centralized authentication services, backups, and Java and LAMP app servers. CUIT is tasked with serving the entire University though so they tend to be a bit more conservative and in their deployments than an organization like ours, whose mission involves keeping up with the cutting edge, sometimes needs. So we run a few of our own servers, using Debian, Ubuntu, and a Xen Hypervisor setup. This lets us run Django, Plone, TurboGears, PostgreSQL, and experiment freely with anything else that comes along that we see potential value in, or even just make sure our applications run with newer versions of software than what CUIT has deployed. Our Xen setup has saved us some major headaches over the last few years and I wouldn&amp;#8217;t go back to a bare OS on hardware setup for our Linux machines unless you put a gun to my head, but, speaking as the one who ends up handling most of the administration tasks, hardware still sucks. Keeping things really reliable and available involves a lot of redundancy and infrastructure and planning. More than we&amp;#8217;d really like to spend our time on. We&amp;#8217;re a small organization, we don&amp;#8217;t have a huge budget for hardware and we don&amp;#8217;t have a full-time sysadmin. We&amp;#8217;ve got a few programmers who do the sysadmin work on the side and (I think I can speak for all of us) we really prefer programming to babysitting server upgrades and losing sleep about what will happen if a NIC fails or a RAID volume gets corrupted or a power supply blows up or (like we had to deal with in December) the chilled water supply to all the server rooms on campus goes offline for a week for maintenance. So we&amp;#8217;ve kept a close eye on and dipped our toes into external hosting solutions in a variety of ways. We have applications and services hosted externally with mixed results. We&amp;#8217;ve also had a couple virtual servers running at Slicehost for a few years. Slicehost saved our chassis during that chilled water affair since we were able to allocate some more space there and move our highest priority applications to Slicehost so they wouldn&amp;#8217;t be affected by the issues on campus. This latest round of dealing with our servers has re-affirmed for us that we&amp;#8217;d like to be moving away from running our own server hardware as much as possible and prompted a fresh surveying of the virtual server hosting landscape. When we first signed up to Slicehost, they were far and away the best deal and one of the most highly regarded VPS hosts. That was a few years ago though and the hosting world doesn&amp;#8217;t sit still. Now there&amp;#8217;s a lot more competition to look at just in the Xen VPS hosting world plus interesting options of a different nature such as Amazon EC2 and Rackspace Cloud Servers. We thought it might be edifying for us to share our research. No one else out there is likely to be in exactly the same situation as us, so this won&amp;#8217;t be directly applicable to anyone else, but perhaps the approach we take to research and evaluation might be helpful to share. Unfortunately, there are such a multitude of options out there that we can&amp;#8217;t directly address everything, so this summary will be limited. In particular, it will really only cover full server hosting. There&amp;#8217;s very interesting stuff going on with application hosting like Google App Engine, various cloud file storage (S3, etc.) and cloud database services, but for now we&amp;#8217;re just looking at services that could be considered a fairly straightforward replacement for our Ubuntu virtual machines (both our locally hosted and Slicehost hosted ones). Those other topics could make for interesting future posts, perhaps. First, a summary of our current situation as a baseline. We have three slices on Slicehost, two 512MB and one 256MB. We pay a total of about $96/month for that. Slicehost includes bandwidth up to 300GB/month for the larger slices (150GB/month for the smaller) in that price and charges per GB after that, but we&amp;#8217;ve never even come remotely close to using our bandwidth allotment. It&amp;#8217;s hard to tell our exact bandwidth usage from Slicehost&amp;#8217;s admin tools, but it seems to be on the order of just a couple GB per month. In part, this is because we have essentially free bandwidth on campus and we&amp;#8217;ve specifically chosen so far to only move applications that we know wouldn&amp;#8217;t be very bandwidth intensive offsite. But, aside from the video related work we do, which will probably stay local for longer than other things, most of our applications shouldn&amp;#8217;t need tons of bandwidth. At CCNMTL, we typically deal with applications that serve a relatively small audience (often a class or two at a time) and don&amp;#8217;t have particularly high bandwidth or CPU requirements on their own. The catch is that we build and deploy a lot of these, they have to be available very reliably, and we have to keep them around for years and years (we have projects that have been active for over a decade now). The main division in our set of options is VPS vs Cloud. There is overlap but the two have slightly different aims. For our purposes, &amp;#8220;VPS hosts&amp;#8221; pretty much all follow the same model as Slicehost. You get a virtual server with a certain amount of RAM, disk, bandwidth, and guaranteed CPU cycles for a fixed monthly rate. Typically you can allocate new servers on demand through a web interface and decommission old ones. Instances are persistent and when you allocate a new one, you get a clean base install to start with. Their primary use-case is someone who wants root on one or more servers for relatively long-term, stable usage and just doesn&amp;#8217;t want to have to deal with hardware. &amp;#8220;Cloud Services&amp;#8221; offer virtual servers at an hourly rate and have programmatic APIs for making new instances, shutting old ones down, etc. You are billed only for the actual hours that the server is using CPU cycles. The primary Cloud use-case is someone who needs burstable capacity. Ie, they occasionally need a lot of CPU or bandwidth but don&amp;#8217;t want to pay the overhead of having all of it running all the time. VPS Hosting Let&amp;#8217;s look at the current VPS world first. There are tons of VPS hosts now, most of them looking pretty much the same. It would take forever to look at all of them, so I&amp;#8217;m narrowing it down to a couple that seem to be the major players and that keep coming up in conversations with people. Slicehost, linode.com, and prgrmr.com. All of them have pretty solid reputations for uptime and reliability. They all support Ubuntu 10.04 Lucid servers (both 32 and 64 bit). There are others out there but they all pretty much line up with one or the other of those. Slicehost is obviously the one we&amp;#8217;re most familiar with. It offers slices with the following configurations: RAM DISK BW PRICE 256 slice 256MB 10GB 150GB $20 384 slice 384MB 15GB 225GB $25 512 slice 512MB 20GB 300GB $38 768 slice 768MB 30GB 450GB $49 1GB slice 1024MB 40GB 600GB $70 1.5GB slice 1536MB 60GB 900GB $100 2GB slice 2048MB 80GB 1200GB $130 3GB slice 3072MB 120GB 1800GB $190 4GB slice 4096MB 160GB 2500GB $250 8GB slice 8192MB 320GB 2500GB $450 15.5GB slic 15872MB 620GB 2500GB $800 Linode&amp;#8217;s offerings: RAM DISK BW PRICE Linode 512 512MB 16GB 200GB $20 Linode 768 768MB 24GB 300GB $30 Linode 1024 1024MB 32GB 400GB $40 Linode 1536 1536MB 48GB 600GB $60 Linode 2048 2048MB 64GB 800GB $80 Linode 4096 4096MB 128GB 1600GB $160 Prgrmr (who are currently at capacity and not taking new customers, but&amp;#8230;) RAM DISK BW PRICE 64MB 1.5GB 10GB $5 128MB 3GB 20GB $6 256MB 6GB 40GB $8 512MB 12GB 80GB $12 1024MB 24GB 160GB $20 2048MB 48GB 320GB $36 4096MB 96GB 640GB $68 VPS Conclusions Benchmarking VPS hosts is a difficult task (since they all do burstable CPU and IO, meaning that while you are guaranteed a certain base level of performance, if the physical machine a virtual server on is not under much load, you&amp;#8217;ll get even more resources). Benchmarks pop up all the time and often claim contradictory results so I won&amp;#8217;t put much stock in them. This one is typical. It&amp;#8217;s pretty thorough, but doesn&amp;#8217;t really avoid the fundamental difficulty of accurately benchmarking virtual server. Linode doesn&amp;#8217;t offer anything smaller than 512MB. This is too bad since I&amp;#8217;ve found that 256MB seems to be more than enough for running quite a few relatively low traffic web applications. We have 512MB slices on Slicehost currently and don&amp;#8217;t come anywhere near using the memory or disk available on them (one slice runs our Project Management Tool, Movable Type, and some other misc stuff while the other runs some TurboGears apps). On the other hand, Linode&amp;#8217;s 512MB option is priced the same as Slicehost&amp;#8217;s 256MB slice. Generally, Slicehost appears to be significantly pricier than its competition. One may observe that Slicehost hasn&amp;#8217;t adjusted their pricing since we started using them several years ago. They&amp;#8217;ve made more larger slice options available, but a 256MB slice a couple years ago was $20 and it&amp;#8217;s still $20 now. Prgmr is very barebones. They have a solid reputation but are clearly aimed at people who know what they&amp;#8217;re doing and don&amp;#8217;t need a shiny control panel. Their founders are authors of a book on Xen and are involved in that community. They&amp;#8217;re obviously cheapest, but it&amp;#8217;s also interesting that they scale down the furthest to 64MB servers. That&amp;#8217;s probably less than we would be comfortable running a Django/Apache/PostgreSQL type setup on, but there might be a use case for it somewhere (irc bots or monitoring or something?). They do seem to have a problem provisioning equipment though. Whether it&amp;#8217;s because they are getting customers too quickly or if they are specifically limiting themselves and trying to stay small is hard to tell. Slicehost went through some of the same things back when we started with them where you had to get on a waiting list for a new slice. Anyway, it means that it&amp;#8217;s a little riskier as far as not being able to fire up new servers as needed there. Anyway, the fact that they&amp;#8217;re currently (as of July 2010) out of space makes them a non-starter for us. We&amp;#8217;ll include them in future searches, but we&amp;#8217;ve got to count them out this time. Now, with a sense of the state of VPS hosting, we can look at Cloud hosting. Cloud Hosting The two main players here are Amazon EC2 and Rackspace Cloud. Amazon practically invented the field and continues to be a primary innovator. Rackspace have been a major hosting and colo provider for a long time, recently purchased Slicehost and appear to be aggressively entering Cloud hosting territory as well. Presumably, they want to have all the options covered. There are others like Ubuntu Enterprise Cloud but nothing with the weight behind it of those two. Pricing with Cloud hosting is much more complicated so it becomes difficult to compare directly. However, since we&amp;#8217;re looking at it as a potential replacement for VPS hosting, we can ignore a lot of things (though the increased flexibility is still something we might want to think about in the future and having it available is nice). Eg, Amazon makes disk persistence and static IP addresses (both things we need) available via separate services (Elastic Block Storage and Elastic IP Addresses) with their own price structures. Let&amp;#8217;s start with EC2. Amazon offers Standard, High-CPU, and High-Memory options. Each of those is then available as Small, Large, or Extra-Large sizes. Our web-apps don&amp;#8217;t tend to be particularly CPU or Memory bound so we probably only are interested in the Standard options. Those look like: Small 1.7GB 160GB $0.085 per hour Large 7.5GB 850GB $0.34 per hour Extra Large 15GB 1.6TB $0.68 per hour Prices drop if you sign up for a full year term (or 3 years, but given how quickly things change, I don&amp;#8217;t think we&amp;#8217;d want to tie down our hosting that far into the future). For reserved instances, you pay a one-time fee up front and then a much lower hourly rate. For the standard options, UP FRONT HOURLY Small $227.50 $0.03/hr Large $910 $0.12/hr Extra Large $1820 $0.24/hr Amazon charges per GB for bandwidth to and from your machines, but it&amp;#8217;s only $0.15 or so per GB and for our purposes would be pennies to a few dollars a month. It is something we do need to keep in mind if we find ourselves wanting to deploy services in the future that have higher bandwidth requirements though. To persist your EC2 instances between boots, you need to set them up to mount EBS volumes. EBS costs $0.10 per GB per month. So, for a Small EC2 instance with 160GB disk, that&amp;#8217;s $16/month. (EBS also charges you $0.10 per million I/O requests, but that&amp;#8217;s going to be pretty negligable). Also, if we don&amp;#8217;t actually need the full 160GB, we can make it smaller and pay proportionally less. Static IP addresses are also extra but are priced so that they&amp;#8217;re practically free except you pay more if the IP address is allocated but not actually being used. I.e., they just try to keep people from hoarding more IP addresses than they actually need. If you just needed a machine for a few hours to run a compute intensive task, obviously EC2 is a great deal since you can just pay a couple bucks for a few hours instead of committing to entire months at a time. We&amp;#8217;re looking at keeping servers running more or less 24/7 and wanting to persist between reboots, so converting to hourly rates is probably the most sensible way to compare. Since we can probably commit to a year at a time, reserved instances make sense for us. So, with EBS storage figured in: RAM DISK PRICE Small 1.7GB 160GB $56/month Large 7.5GB 850GB $248/month Extra Large 15GB 1.6TB $486/month If we don&amp;#8217;t need the full size of those disks to persist and limit it to, say, 20GB, those prices drop to $42, $177, and $328, respectively. Now we can easily see that their &amp;#8220;Small&amp;#8221; instance is actually priced competitively with Linode (and even gives you quite a bit more disk) and significantly cheaper than Slicehost&amp;#8217;s 1.5GB slice. There&amp;#8217;s more work involved in setting up an EC2 instance as far as learning Amazon&amp;#8217;s tools and APIs and building an AMI, but the flip side of that is that once we&amp;#8217;ve done that work we&amp;#8217;re closer to being able to do really fancy things like automatic scaling (a server detects that it&amp;#8217;s under heavy load and spawns another EC2 instance to help out) plus we get to make use of other parts of Amazon&amp;#8217;s infrastructure like SQS, S3, Elastic MapReduce, Elastic Load Balancing, SimpleDB, CloudFront, SNS, CloudWatch, RDS, etc. Pre-built EC2 images (AMIs) are also becoming available as appliances. Furthermore, there is a lot of mindshare around the AWS APIs with nice Python libraries available and ream upon ream of tutorials written on how to build your apps around them. Our primary issue with EC2 is that it doesn&amp;#8217;t currently scale down as far as we&amp;#8217;d like. A machine with 1.7GB of RAM is overkill for a lot of our needs but that&amp;#8217;s as small as you can provision through EC2. We really like to keep things very granular and compartmentalized e.g., by keeping Django apps on a separate server from TurboGears apps from LAMP apps from Plone apps. That lets us do updates with fewer worries about complex dependencies and breaking things that are seemingly unrelated. This means that we strongly prefer running multiple smaller virtual servers that each have a very specific purpose to running one large one with a bunch of different things. With VPS hosts, we can do that with 256MB servers (or even 512MB) at $20 or less per server. With EC2, it&amp;#8217;s pretty much $40 - $60 each so that adds up quickly. The other small issue with EC2 is that EC2 instances are 32-bit until you get into the Large or Extra Large size. It&amp;#8217;s not a big deal, but our current Python oriented servers and dev environments are 64-bit and it occasionally requires some tweaking to deployment scripts or repackaging of eggs to deal with more than one type of platform. With that in mind, let&amp;#8217;s look at RackSpace Cloud. Their offerings: RAM DISK HOURLY PER-MONTH 256 MB 10 GB 1.5¢ $10.95 512 MB 20 GB 3¢ $21.90 1024 MB 40 GB 6¢ $43.80 2048 MB 80 GB 12¢ $87.60 4096 MB 160 GB 24¢ $175.20 8192 MB 320 GB 48¢ $350.40 15872 MB 620 GB 96¢ $700.80 Definitely some appeal there. Disk is a bit less and they don&amp;#8217;t offer all the additional services that Amazon does and there aren&amp;#8217;t as many third party libraries, etc to deal with their API, but it appears to be a pretty simple, sane REST &#43; JSON (or XML if you prefer) sort of deal, so it shouldn&amp;#8217;t be too bad. Rackspace also does the more VPS-like burstable CPU while EC2 gives you more explicitly limited CPU. EC2 is extremely predictable in terms of performance, but Rackspace has that &amp;#8220;you usually get more than you pay for&amp;#8221; appeal. Obviously, it&amp;#8217;s also notable that Rackspace Cloud does scale down the the 256MB level, which is something we care about. Ian Bicking&amp;#8217;s Silver Lining project, which is sort of Google App Engine meets ccnmtldjango meets pre-built AMI is currently focused on deploying to Rackspace Cloud. I&amp;#8217;m not in a big hurry to jump on Silver Lining, but it&amp;#8217;s interesting to note. Overall Conclusions To recap, here&amp;#8217;s a very rough comparison of all the options at (or as close to as is available) the 512MB size: Host RAM Disk BW Cost Slicehost 512MB 20GB 300GB free $38 Linode 512MB 16GB 200GB free $20 Prgrmr 512MB 12GB 80GB free $12 EC2 1.7GB 20GB $.15/GB $42 Rackspace 512MB 20GB $.22/GB $22 Slicehost is seriously looking like the least attractive option at the moment. What they have going for them is that we are already on Slicehost and we haven&amp;#8217;t really had any major issues with them. As far as expanding our external hosting though, it doesn&amp;#8217;t seem to be where we want to be going. Even without moving to Cloud hosting, it looks like we ought to test out Linode for future VPSes. Prgrmr is interesting but if they can&amp;#8217;t provide new images quickly, that&amp;#8217;s pretty limiting. Whether we want to test the waters of EC2 or Rackspace Cloud is a more strategic question. Scaling on demand is something that we so far haven&amp;#8217;t needed all that much in the past. On the other hand, we&amp;#8217;ve been developing for years with it always in the back of our heads that we just can&amp;#8217;t scale on demand. E.g., when a situation pops up that we really could use more resources for a short time (the Millennium Village Simulation or Collateral Consequences launches come to mind), we just haven&amp;#8217;t been in a position to dynamically add a new machine and load balance to it so we&amp;#8217;ve concentrated instead on making due with what&amp;#8217;s there and keeping things hobbling along until traffic decreases. Investing the time in getting comfortable with these Cloud hosting services could change that and we could find that we start developing and deploying differently. Amazon&amp;#8217;s offerings are particularly intriguing in that respect. They pretty much encourage you to decentralize, load balance, and overall structure things in very flexible, reliable ways. Rackspace Cloud has a bit more of a VPS feel to it and is worth checking out for that reason. It kind of feels like a Cloud/VPS hybrid and I think that might be what we want. I.e., it&amp;#8217;s kind of like Slicehost but cheaper and with an API that we can use to load our own images on demand (none of the VPS hosts let you upload your own image; you have to use one of their supported options. EC2 and Rackspace both have pretty specific requirements on their images, but you are able to download them to keep your own backups, clone new images off old ones, etc). Rackspace Cloud is probably the lowest risk Cloud hosting option for us to check out while EC2 has a potentially higher payoff in terms of access to the whole AWS ecosystem. With Amazon and Rackspace we also have the option of only paying by the hour for our experiments so it&amp;#8217;s mostly a matter of how much programmer time we can spare.]",
   "tags": ["cloud", "EC2", "hosting", "rackspace", "slicehost", "vps", "xen"]
},
{
   "url": "/articles/mediathread-nyc-django-meetup/",
   "title": "MediaThread @ NYC Django Meetup",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[Last night Sky and I presented MediaThread at a Django meetup in NYC. We were invited to give a full presentation after a lighting talk back a few months ago. The event was held at Huge with about 20 people attending from a variety of backgrounds. The participants were engaged and involved, asking questions about our implementation, process, and student usage. We also heard a report back on the European DjangoCon and broke bread (and beer) with local Djangonaughts.]",
   "tags": ["django", "mediathread", "presentation"]
},
{
   "url": "/articles/mediathread-help-us-jump-start/",
   "title": "MediaThread: Help us jump-start a community",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[MediaThread is a media analysis communication platform we announced back in January. &amp;nbsp;At the moment it sports a number of central features: annotating images large images on any web page, Flickr, and some specific collections like ArtStor (for subscribers)clipping video into an annotation YouTube, quicktime, flv, flv pseudo-streaming, realmedia, h264, and preliminary ogg (when the browser supports it)embedding your image and video annotations into a multimedia essaydiscussing collected images and video (we call them assets) in a space where you can also embed annotationsWe find this kind of communication affords and encourages deep analysis and &#34;brings the laser pointer to the essay.&#34; Instead of just referencing a video and describing the scene, you can embed the exact moment and let the reader view the evidence directly and immediately. We would love for this platform to grow beyond the walls of Columbia. &amp;nbsp;Fostering a community for a new open-source project is always a bit of a challenge, so please contribute with questions, suggestions, code, experience or insight. &amp;nbsp;The MediaThread forum will not just be for developers, so if you are using MediaThread, then tell us about your experience. Perhaps it&amp;rsquo;s worth relating a bit of history about how MediaThread came to be: MediaThread grew out of a grant with WGBH to archive video from their documentary about the Vietnam War. &amp;nbsp;WGBH already had a nice platform for searching and browsing their video collection. &amp;nbsp;They also had internal media analysis tools, but the communication structure did not match well with using these tools in a class setting. &amp;nbsp;In a class, you want to see what your fellow students are doing automatically, but not what people in other classes are doing (sometimes it&amp;rsquo;s important that it&amp;rsquo;s impossible, even) We considered working on their open source code, but I had just come off a project that integrated image annotation into a large archive. &amp;nbsp;We pulled it off, but the complexity of supporting both customized annotation and collection search/browse capability seemed to give the site too many features. Figuring out how to keep the site simple (both for users and programmers) was a big challenge. There was another issue. &amp;nbsp;Some of our faculty partners wanted to use additional video in their class that was not available in the WGBH archive. &amp;nbsp;One option would be to include the video inside our instance of the WGBH site. &amp;nbsp;However, this yields more problems: making it clear what came from WGBH and integrating our separate videos with search/browse. Out of these challenges was born the idea of making a separate site just for the course-related media-analysis and discussion. At the beginning, the WGBH archive site had an &amp;lsquo;analyze this&amp;rsquo; button on an asset page. The user would click the button and the asset would be ported over to their MediaThread course. &amp;nbsp;For collections that are conscious of MediaThread, that is still possible, however, now we encourage using a bookmarklet (http://en.wikipedia.org/wiki/Bookmarklet) which can be run on many archives across the World Wide Web. This approach has some incredible advantages. Instead of the annotation and analysis tools being tied to any single collection, MediaThread allows users to integrate assets from many collections and sources into a single project. Between other projects, I&amp;rsquo;ve been working on MediaThread for about a year now. &amp;nbsp;We&amp;rsquo;ve had other media annotation projects at CCNMTL (Columbia Center for New Media Teaching and Learning), but MediaThread seems to have some wind in its sails as a platform, and we&amp;rsquo;re excited for others to try it out and lend their experience to its development. &amp;nbsp;Please join us. http://github.com/ccnmtl/mediathreadhttp://twitter.com/CUmediathreadhttp://groups.google.com/group/mediathreadhttp://ccnmtl.columbia.edu/mediathread]",
   "tags": ["annotations", "mediathread", "multimedia"]
},
{
   "url": "/articles/pycon-10-recap/",
   "title": "PyCon &#39;10 Recap",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[Here are some belated highlights from PyCon2010, straight from the Ministry of Silly Talks. The talks are all published on the PyCon website and all the video are now posted and organized at http://python.mirocommunity.org. This year I participated on the PyCon video team, and helped record the tutorial sessions. After being burned a few years ago trying to get volunteers to do post-production for hundreds of hours of conference video, PyCon borrowed a few tricks (and people, and software) from the Debian Linux community and started mixing their conference captures - live, in real-time. Now, using an inexpensive laptop as a mixing board, the audio, video and VGA streams are all mixed and captured using the libre DVswitch. This kind of setup used to cost thousands of dollars just a few years ago, and it was pretty amazing to participate in a video production project at this scale. Most of the videos were published within a day or two of being recorded. Projects like OpenCast are poised to continue to improve their encoding and publishing workflows - a complex problem with so much footage being processed so quickly. The a/v squad documents their process - these are the best notes I have found so far. Some of my favorites: These two talks really helped me understand the nosql landscape - What every developer should know about database scalability To relate or not to relate, that is the question A tutorial on the alphabet soup of standards driving data portability and web programming Hacking the Social web w/ python (video) This was my first exposure to VisTrails: A Python-Based Scientific Workflow and Provenance System - a Yahoo Pipes graphical programming environment, but one that saves a full versioned history of your changes, and is designed to produce graphical representations of the data. The tool is meant to help researchers graphically develop visualizations of their data, but also helps keep track of which data any visualization is based on. Could this represent an emerging programming paradigm? This was a great talk about diversity, from a practical/pragmatic perspective Diversity as a Dependency making the case that organizational innovation improves with diverse participants. And a talk presented by a partner of ours working on Professor Modi&#39;s Network Planner - How Python is guiding infrastructure construction in Africa There was plenty more - lighting talks, keynotes, django, gis, pinax, turbogears, plone, zope, and lots of lang level stuff - please share any treasures (and recipes) you discover.]",
   "tags": ["diversity", "dvswitch", "nosql", "pycon", "python", "video", "vistrails"]
},
{
   "url": "/articles/nodejs-frameworks-review/",
   "title": "Node.js frameworks review",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[This is a strongly-opinioned fly-by review of&amp;nbsp;the evolving web framework ecosystem&amp;nbsp;for&amp;nbsp;Node.js. I&amp;rsquo;m a big fan of Javascript, and write a lot of it for browsers. &amp;nbsp;So I&amp;rsquo;ve been excited about the prospects of server-side javascript which opens up a lot of possibilities for sharing code on the client and server side. Node.js seems to have exploded server-side javascript development, as it was fast, easy to setup, and provided a niche-advantage in handling requests asynchronously. But Node.js is a low-level runtime which brings a good standard library and scripting environment for deployment. We don&amp;rsquo;t want to re-write cookie and session handling for every project. &amp;nbsp;For web development we need a framework that organizes code and brings the old lessons that Rails brought to projects like Turbogears and Django in python. I have some biases. I&amp;rsquo;ll just list them up front:I&amp;rsquo;m coming from Django and these days do most server-side development in python. In most things, the more django-y the better.However, I&amp;rsquo;d like to keep Node.js&amp;rsquo;s cool asynchronous structure, and I&amp;rsquo;d rather idiomatic Javascript than forcing it to look like another language.I hate polluting global namespaces. This is a deal-breaker for me.I want some help setting up sessions, but we&amp;rsquo;ll be developing our own login plugin (Wind for use at Columbia), so I&amp;rsquo;m paying special attention to that and how plugins integrate. Terminology&amp;ndash;like in Django, I&amp;rsquo;m going to refer to a Controller as the route-handling parts, and a View as the page handling code (separate from templates). It seems like most Node.js frameworks see it this way too (along with the original MVC Scheme understanding). Let&amp;rsquo;s get started: Chainweb page says &amp;ldquo;deprecated in favour of ejsgi&amp;rdquo;&amp;ndash;see below. Coltranelast-commit: July 20, 2009looks abandoned. Djangodelast-commit: March 19, 2010pollutes global namespace: &amp;nbsp;NOsession model: NONEtemplate default: Django templatesThere doesn&amp;rsquo;t seem to be much code to simplify writing Views. This project seems to focus on the template side of things. EJSGIlast-commit: February 18, 2010pollutes global namespace: NOsession model:&amp;nbsp;NONEtemplate default:&amp;nbsp;NONEpretty low-level, but has login examples. Views are passed a request object, and return a response object (that then gets processed and sent as the Node.js response object. ExpressThis is clearly one of the most advanced projects. I would probably try to work with Express if it weren&amp;rsquo;t for the global namespace thing. It also obscures the asynchronous features in Node.js. last-commit: May 15, 2010 (3 days ago)pollutes global namespace: YESsession model: Store.Memory (this.session)template default: Haml (%h1, %p)comment: Class() -basedView details:&amp;nbsp;&amp;lsquo;this&amp;rsquo;&amp;nbsp;object in views exposes plugins which can be called. That undermines the easy use of Node&amp;rsquo;s asynchronous features. It forces asynchronous actions to be done in the View code, which could be a good thing.GeddyThis even provides a &amp;lsquo;geddy-gen&amp;rsquo; to do a build-out of a project like Rails/Django/etc. It also has models and data-validation. Very nice. last-commit: May 16, 2010 (15 hours ago)pollutes global namespace: NOsession model: store or memorytemplate default: EJS (javascript templates)&amp;nbsp;&amp;lt;% and %&amp;gt; &amp;nbsp;&amp;lt;%= and %&amp;gt;View details:There&amp;rsquo;s a this.cookies and a this.session available to the view.JIMI&amp;nbsp;(based on Djangode)This project went up as I was reviewing all the modules. It says something about how fast this field is changing. This post will probably be out of date in a month. last-commit: May 18, 2010 (1 day ago)pollutes global namespace: NOsession model: NONE so fartemplate default: django templates (though the sample app doesn&amp;rsquo;t use it yet)Jimi seems to be fleshing out handlers similar to Django&amp;rsquo;s structure: urls.py, static directory, templates directory, and a views.js View details: we have convenience methods like redirect(), etc.&amp;nbsp;As much as I like Django, I think this is again obscuring Node&amp;rsquo;s great asynchronous possibilities.JS.ioThis doesn&amp;rsquo;t seem to be a traditional web framework. It&amp;rsquo;s seems to be more like Comet&amp;ndash;something that lets you do javascript-rpc stuff. MVClast-commit: March 27, 2010pollutes global namespace: NOsession model: NONEtemplate default: EJS, &amp;lt;% and %&amp;gt; &amp;nbsp;&amp;lt;%= and %&amp;gt;View details:this.renderText()mvc.addToContext( {DICT} ) adds to &amp;lsquo;this&amp;rsquo; in views like middleware&amp;ndash;a similar approach to plugins in Express.There doesn&amp;rsquo;t seem to be access to the request and response objects in the Views, which will make it difficult to &amp;lsquo;cheat when you have to.&amp;rsquo;NerveI think this is my favorite at the moment. Even if it&amp;rsquo;s a &amp;lsquo;microframework&amp;rsquo; it contains the important pieces, and will let me scaffold my own structure for views. last-commit: May 15, 2010 (2 days ago)pollutes global namespace: NOsession model:req.session &amp;nbsp;(memory/cookie, at the moment unconfigurable)decorates http.IncomingMessage&amp;nbsp;with get_cookie(), ,set_cookie(), get_or_create_session()template default: NONE (but link to template.node.js)comment: nice for a view framework View details:Passes views the request, response objects along with the matched elements from the Controller regex. Pinturalast-commit: May 15, 2010 (2 days ago)pollutes global namespace: NOPintura is part of some monolithic Dojo thing, which probably makes a lot of sense if you use Dojo. Everytime I&amp;rsquo;ve checked out Dojo, I find a buggy, disorganized, constantly refactored ill-documented mess. People keep raving about it, so it must be good. I&amp;rsquo;m just too stupid to figure it out. ConclusionThe most mature projects at the moment both come inspired from Rails projects. Those are Express and Geddy. A more full-featured Django clone looks inevitable in Jimi or some future incarnation. I&amp;rsquo;m going to be starting out with Nerve, but if I start needing more that what&amp;rsquo;s there already, I might jump over to Geddy. Anyway, hope this helps anyone else taking a peak into this early Node.js community which is growing every day.]",
   "tags": ["javascript", "nodejs"]
},
{
   "url": "/articles/final-cut-pro-xml-parsing/",
   "title": "Final Cut Pro xml parsing",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[MediaThread, our new media analysis platform, previously codenamed &#39;Mondrian&#39;, now supports Final Cut Pro&#39;s XML Interchange Format. I split the XMEML code out as a library that might be useful to others. Get your free (as in BSD) code here:&amp;nbsp;http://github.com/ccnmtl/xmeml This library helps create an XMEML file based on references to clips from other XMEML files. &amp;nbsp;It was born by getting two problems to cancel each other out. This should help solve a problem the CCNMTL video team frequently encounters. Educational Technologists and professors choose specific sequences from raw video footage.&amp;nbsp;The video team then takes the selected clips and composes a finished product. How do they communicate clips? &amp;nbsp;It&#39;s generally pretty awkward, and mostly emails with a long list of timecodes goes back and forth which leads to tedious data input on both sides. We&#39;d always dreamed that the video team could use MediaThread for this purpose. Faculty could clip their video in MediaThread, which would then export an XMEML file that the video team could use. The first problem was that XMEML contains many variables that MediaThread wouldn&#39;t know such as filename and path and details about audio tracks. &amp;nbsp;None of these variables are contained in the small video encoded for the web--they remain in the raw footage stored by the video team. The second problem came when we tried using MediaThread anyway. Faculty would choose their clips, but the timecodes recorded from the web video track was different from the timecodes for the raw footage. The web video didn&#39;t just start at a different time. Even the &#39;raw&#39; footage that was being presented to the faculty selecting the video had already been clipped and rearranged. We solved the problem by taking the original XMEML file that produced that video and using it to spit all the unknown variables right back into the XMEML file. Thus, we no longer had to guess the variables and the original XMEML file contains all the information about how the raw footage was rearranged. There are still some issues. &amp;nbsp;I&#39;m not importing transitions at the moment, and not even sure what the best thing to do with them is. &amp;nbsp;When they&#39;re split by a clip, it&#39;s ambiguous what should happen. Patches welcome!]",
   "tags": ["bsd", "final cut pro", "free-as-in-speech", "video", "xml"]
},
{
   "url": "/articles/drupal-multisite-manager/",
   "title": "Drupal Multisite Manager",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[After some prodding from the community, I&#39;ve finally published Drupal module Multisite Manager 6.x-1.0. I originally developed this module to fill a sweet spot that was missing at CCNMTL two years ago--allowing our Educational Technologists to just click a button to be able to create a new site to prototype content, or experiment with Drupal&#39;s features. Drupal is a great platform that makes this possible, but then allows a developer to add features later--significantly, after content and some site architecture has been worked out. At CCNMTL for small projects, we&#39;re using more hosted solutions and specific tools like Wikispaces and Wordpress MU, rather than general CMSs like Drupal and Plone. Our farm is still on Drupal 5.x, and it seems migration is near impossible due to module dependencies and unclear upgrade paths. Nonetheless, it wasn&#39;t clear at the time this would always be the case. So when Drupal 6.0 came out, I upgraded the code to work for Drupal 6.0. We do not use this code ourselves, so I wasn&#39;t very confident about it&#39;s usefulness; I left it marked as development branch. This hasn&#39;t stopped it from getting about three hundred downloads a week. It seems my concern about bugs was unwarranted in this case. Even though we don&#39;t use it ourselves, at the time, it was still the easiest way to see the differences between Drupal 5 and Drupal 6. Now Drupal 7 is coming, and I can&#39;t think of a better way to keep tabs on the community besides just writing some code, and seeing how it does.]",
   "tags": ["drupal"]
},
{
   "url": "/articles/one-of-the-primary-tenets/",
   "title": "Test.AnotherWay: JavaScript Unit Tests Made Easy",
   "author": "Susan Dreher",
   "date": "",
   "content": "[One of the primary tenets of agile development is test first, test often. After working in a small XP shop doing mobile development, I came to believe strongly that quality code hinges on a test-driven approach. Coders, impatient with paper specs and endless product meetings, often rush to their keyboards and push out half-baked, poorly implemented solutions that don&#39;t meet anyone&#39;s needs. Writing tests -- especially in a test-first approach -- provides time for thoughtful inquiry into an application&#39;s overall design and specific functionality. The coder can express herself in her own comfortable environment and language. The resulting tests become permanent artifacts, able to verify functionality as the application is enhanced and refactored. And, in less altruistic, more self-serving terms: good tests mean good code, and good code makes the coder look good. Why wouldn&#39;t you want to write tests? Still, I was a little apprehensive when asked to setup a test infrastructure for the Mondrian JavaScript components. (Mondrian is our snazzy new web-based, multimedia, annotation environment). I&#39;ve tackled many server-side testing tasks, but have managed to circumvent the swampy land of JavaScript. JavaScript generally does not lend itself to testing. Most JavaScript code I&#39;ve seen is poorly organized, fragmentary and tightly-bound to the browser. I&#39;ve often lamented the lack of good JavaScript testing tools, but also was loathe to tackle the seemingly messy, difficult task. The Mondrian JavaScript codebase, luckily, is quite object-oriented and well-organized thanks to Schuyler Duveen, the lead programmer/architect on the project. Sky suggested I take a look at Test.AnotherWay, the JavaScript test tool used by the OpenLayers community as a starting point. Written by Artem Khadoush, Test.AnotherWay turns out to be perfect for the job -- small, simple and easy to setup. Sold! Khadoush&#39;s stated goal is an &#34;effortless setup&#34; and he definitely achieves that goal. For a small project, this test infrastructure is surprisingly complete. In addition to the expected assertions, the test runner also handles asynchronous calls and can record and playback mouse events. The basic setup involves downloading a main test runner (run-tests.html) into your web application and registering your test file (list-tests.html). Constructing a test is straightforward (and familiar). Execute functionality within a function named testX, and then verify results with a variety of assertions. I wrote a simple test in 5 minutes. I then wrote more complex tests for our video interfaces (YouTube, FlowPlayer and Quicktime). Each player is put through its individual paces -- play, start, stop, seek, pauseAt(x seconds), duration, time. The players all have distinct personalities, and require some handholding. Test.AnotherWay helped out immensely by having facilities to run the players in their own iFrames and to &#34;delay&#34; assertions while the player loaded or fast forwarded to a particular time point. Take a look at the YouTube test source for a good run through. Once the tests were in place, I was able to complete some much needed cross-browser testing. I immediately found and resolved bugs in IE 8 and Safari. I just completed some refactoring to make the code more consistent between players. Running the tests after each step made me feel confident I hadn&#39;t broken anything. There are still many more tests to write, but the lessons (re)learned are clear: writing tests should always be a priority, even if the implementation looks rocky and difficult. The payoff is incredible.]",
   "tags": ["agile", "javascript", "testing"]
},
{
   "url": "/articles/collaborative-futures/",
   "title": "Collaborative Futures",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[Everything is speeding up these days, even the authoring of books. Some information society researchers we know (including some of our friends from Eyebeam, Creative Commons and Shift Space) locked themselves up for a week in Berlin, and came out the other end with a print-ready book on the future of collaboration - Collaborative Futures. Even though I didn&#39;t travel to Berlin, the authorship of the book was radically distributed and some of my writing made it into the final cut. A portion of essay I wrote last fall for a sociology seminar on the future on a (brief) history of version control systems and the significance of distributed version control systems made the cut. The book will be released under a creative commons license, but they are also doing a print run of hard copies which will be available starting at the launch party, March 4th. Pre-order hard copy here (digital copy is available here).]",
   "tags": ["collaboration", "git", "process", "versioncontrol"]
},
{
   "url": "/articles/composition-with-video-images/",
   "title": "Announcing Mediathread: Composition with Video, Images, and Text",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[We are very excited to announce the release of our latest iteration on a web-based, multimedia, annotation environment - code named: Mondrian Mediathread ( source code ). Mediathread builds on the strengths and experiences of our long history of annotation projects here at CCNMTL. Mediathread is a collaborative multimedia analysis environment that supports deep critical exploration of primary multimedia source material, i.e. participatory education, research, democracy, and culture. The Mediathread platform supports a robust access control model with multiple analysis spaces and a variety of workflows (solo projects, collaborative projects, versioning, private projects, public projects, etc). The community portal also organizes streams of activity notifications to help the participants track each other&#39;s (net)work. Participants in the analysis space collect multimedia assets from around the web, clip/annotate these assets, organize their clips, and create a multimedia composition where their clips are directly embedded inline in their analysis/argument. The upcoming release supports video clipping (quicktime, flowplayer, and youtube), and drawing on images (using the fabulous OpenLayers viewer). Mediathread was designed as a mashup, and the software does not provide asset management services. Instead, Mediathread operates upon assets that remain on their original server media, analogous to the way del.icio.us operates on links. One day we hope to be able to annotate (and analyze) anything at the other end of a URL. The targeted media does not need to &#34;know&#34; about Mediathread - assets can be brought into Mediathread manually, or with a convenient bookmarklet. Of course, if you are are in control of the source archive, it is simple to add an &#34;Analyze This&#34; button directly to the archive for convenience. Mediathread is built on Django &#43; the SherdJS javascript framework - also developed here at CCNMTL for Mediathread, but decoupled with the intention of using against alternate backends. This semester our Mediathread platform will be supporting the Vietnam Digital Library courses as well as this year&#39;s Digital Tibet course (for a glimpse at these gorgeous images, see http://digitaltibet.ccnmtl.columbia.edu/). Publishing this application has really motivated us to clean up the messy corners of the codebase, in ways that this post on Why Do We Write? articulates nicely. We have also finally straightened out our stories around distributed version control (git submodules vs. subversion externals will be the the subject of a future post). There is still plenty of work ahead of us, and a challenging roadmap towards purposeful, scalable, self-service, collaborative multimedia analysis. Special thanks to everyone involved in supporting these ongoing efforts, including all of our staff, WGBH, and IMLS.]",
   "tags": ["annotations", "django", "html5", "javascript", "mashup", "openlayers", "video"]
},
{
   "url": "/articles/gource-visualizing-work-on-projects/",
   "title": "Gource: visualizing work on projects",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[Gource is a fun tool that takes your source repository, and outputs a video showing your developers as little gnomes dancing around the source tree building the program. I think it&amp;rsquo;s a great visualization to demonstrate to the less technical members of our organization and the world, what a programmer does on a daily basis. The source-tree is simple enough to be graspable as program components, yet reflects some of the complexity we handle. Thus it shows pretty vividly how fractured a programmer&amp;rsquo;s attention can be, and why we need time and space to develop. I&amp;rsquo;ve run this on a couple of our own projects. I think this could also be useful in visualizing the activity of classes; for example in a wiki. The little gnomes are contributors. The dots are files (colored by file-type) and the rays coming represent changes to the file by the contributor. This doesn&#39;t reflect perfectly how much one contributes to a project--often the hardest parts of a program take up very little space. But it doesn&#39;t mean nothing either. At the beginning of the Mondrian project in May 2009 you can see Ethan filling out pretty much the entire application. Besides the initial skeleton, I don&#39;t contribute anything of substance until late June (and then I pick up, I think :-) After the videos, I have some details about how I generated these. Mondrian Worth MilleniumVillage CCNMTL&amp;rsquo;s Whole SVN Repository History (long) How I made these After installing gource, most of these were generated in a process that looks something like this. I downloaded images of each developer (from our staff page) and then for each project (checked out from Svn) I did: cd my-svn-project svn log -verbose -xml &amp;gt; my-project.log python svn-gource.py -filter-dirs my-project.log |grep -vi mochikit|grep -vi tiny_mce|grep -vi yui &amp;gt; gource.log gource -640×360 -stop-position 1.0 -user-image-dir ~/pictures/ccnmtlavatars -hide-filenames -a 100 -s 0.5 -log-format custom gource.log -output-ppm-stream - |ffmpeg -y -b 3000K -r 60 -f image2pipe -vcodec ppm -i - gource.flv]",
   "tags": ["gource", "millenniumvillage", "mondrian", "svn", "visualization", "worth"]
},
{
   "url": "/articles/encouraging-the-robot-takeover/",
   "title": "Encouraging the robot takeover",
   "author": "Anders Pearson",
   "date": "",
   "content": "[I subscribe to a lot of programming oriented newsfeeds so I seem to have a steady stream of links coming in that I find interesting and like to share with the rest of the dev team. I usually post the links in our IRC channel as a nice way of informally spreading them around to the other developers who might be interested in them. It&#39;s nice and simple. Just copy and paste the URL into my IRC client, maybe followup with a line explaining what the link is, and then the other developers will see it and can check it out and we can discuss it. For the use case of &#34;share a link with a small community and then discuss&#34;, I think it&#39;s much easier than sending an email, and really even easier than twitter (plus no obnoxious URL shorteners obfuscating things). The downside is that not everyone&#39;s in the IRC channel all the time. We have a bot that logs the channel, but I don&#39;t think many of us rigorously check the IRC logs every time we&#39;ve been out of the channel for a bit. So sometimes I have a link sitting in a tab in my browser that I want to share with the group but I know that one of the developers who would be particularly interested in it happens to be offline at the moment. So I frequently end up accumulating tabs in my browser until there&#39;s sort of a quorum of developers in the channel. That feels like a step back. I might as well be sending a message to a mailing list. What is this, 2003? This hit me particularly acutely last Friday when it happened that about half the dev team was out that day, and I had some particularly juicy links I wanted to share. I started pondering the problem and came to the conclusion that what I wanted was for the links I (and anyone else) post in the IRC channel to be automatically collected and aggregated into an RSS feed that we could all then subscribe to. So anyone who misses a link in the IRC channel would have a good chance of noticing it come up in their newsreader of choice when they come back. The most sensible way to accomplish that was probably an IRC bot that would sit in the channel, look for URLs and then re-post them to a Delicious account created specifically for that purpose. Sky, the reigning IRC bot expert (having set up our ops-keeping and logging bots) was out that day so I couldn&#39;t ask him about it. Ethan, one of the few developers in that day, suggested checking out phenny, which is a relatively simple IRC bot written in Python. Phenny seemed to be just what I needed. I got a phenny-bot running in a couple minutes and started to poke at it to see how it worked. It&#39;s built on top of Python&#39;s asyncore and async_chat libraries, which are not the most intuitive libraries ever, but are efficient and reliable, and it appeared that Phenny had actually done a good job of making a much more intuitive API accessible. So I could add functionality to Phenny without having to really deal with any of the asyncore madness which surely would&#39;ve turned my brain into mush on a Friday afternoon. Even I was impressed when, about ten minutes and an &#39;import pydelicious&#39; later, I had my own IRC bot that would post URLs up to a delicious account. All I had to write: from pydelicious import DeliciousAPI from datetime import datetime def saveurl(phenny, input): &#34;&#34;&#34; logs a url so others can check it out&#34;&#34;&#34; parts = input.split(&#34; &#34;) url = parts[1].strip() title = &#34; &#34;.join(parts[2:]).strip() # the simplest of sanity checks if not url.startswith(&#34;http://&#34;) or not title: return poster = input.nick now = datetime.now() comment_url = (&#34;http://quimby.ccnmtl.columbia.edu/ircbot/web/&#34; &#34;?y=%04d&amp;amp;m=%02d&amp;amp;d=&#34; &#34;%02d#%04d%02d%02d%02d%02d%02d&#34; % (now.year,now.month,now.day, now.year,now.month,now.day, now.hour,now.minute,now.second)) a = DeliciousAPI(&#39;phennyccnmtl&#39;,&#39;nottherealpassword&#39;) a.posts_add(url,title,&#34;posted by %s: %s&#34; % (poster,comment_url)) saveurl.commands = [&#34;url&#34;] saveurl.example = &#34;.url http://www.example.com/ Title For Link&#34; That gets dropped into phenny&#39;s &#39;modules&#39; directory and it&#39;s done. When a user in the channel types &#34;.url http://www.example.com/ Title For Link&#34;, Phenny will notice the pattern and post that link up to its Delicious account. Now I feel like I have a bit more of an idea of why the zombie botnets all use hidden IRC channels as their control mechanism. If you&#39;re comfortable learning a couple basic commands, you can easily control a whole army of these bots. IRC&#39;s built-in distributed, scalable, relatively reliable architecture is proven and tested and you get it pretty much for free when you create a channel. Phenny gets the job done, but it&#39;s far from perfect. To add a new command to Phenny, you have to put it into the &#39;modules&#39; directory in Phenny&#39;s source directory. There&#39;s no real API for registering plugins beyond that. It literally does an opendir(&#39;modules&#39;) and tries to import each .py file it finds there. This means that to add a custom command, you have to fork Phenny. If there&#39;s ever a security update to Phenny or something, we&#39;re on our own to merge it in. I&#39;ll live with it though. I have a fondness for simple tools that just get the job done without making things more complicated than they need to be.]",
   "tags": ["bots", "irc", "python"]
},
{
   "url": "/articles/moodling-around-with-moodle/",
   "title": "Moodling around with a Moodle mp4 filter",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[This semester we partnered with the Earth Institute and launched an environment to support the new international Masters in Development Practice. The environment is intended to support curricular interactions as well as less formal student interactions (i.e. social networking) and our first iteration is built on a combination of Moodle and Elgg. We are just dipping into Moodle development and ran into a situation where we wanted to customize the video player we were using to display the video. I ended up writing a small moodle filter which automatically processes .mp4 files and outputs the appropriate embed code. The code is now available at on github: moodle-mp4filter.]",
   "tags": ["moodle", "opensource", "video"]
},
{
   "url": "/articles/annotating-conversations/",
   "title": "Annotating Conversations",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[For thousands of years critical and scholarly discourse around text has revolved around citation and reference. What might this kind of discourse look like around multimedia - html text, images, audio, and video? This question is a central theme in our technical work here at CCNTML, and a variety of our projects have taken a pass at this question from one angle or another. My colleagues have also taught me the importance of designing these kinds of features in ways that encourage students to critically engage with the source materials they are studying. How can we facilitate the marshaling of multimedia sources as evidence to support an argument or hypothesis? Over the years I have become convinced that the &#39;annotation&#39; abstraction represents the heart of participatory media and social networking systems. Central to these systems is the symmetry of the relationships. It may be as important to be able to look up all the annotations by a particular user, as it is to look up all the annotations on a particular object, as it is to look up all the annotations which have something in common and tie them back to the contributor and the original object. Technically speaking, fine-grained, user created content annotations are an abstraction intended to capture engagements which involve attaching information to content on a per-user, per-object basis. Many metatdata schemes/vocabularies are per-object (e.g. categories in Wikipedia), while folksonomic systems like flickr and del.icio.us annotate content (images and links, respectively) on a per-user, per-object basis. A robust user contributed content annotation framework would provide the core services to support a number of increasingly critical features which community driven sites rely upon. Over the years, we have come across various solutions to these kinds of problems, as well as developed some of our own. This semester, we partnered with WGBH in our latest take on a video clipping and analysis environment, Project Vietnam, launched this week (after a pilot run this summer) and we plan to continue advancing and freely releasing this work. In this analysis environment, a &#34;clip&#34; is just the annotation of an in-point and an out-point on the source stream. Project Vietnam is very similar to our VITAL software but is designed as a mashup, meaning we can analyze clips from across the internet - in theory, any video at the other end of a url (in practice, only quicktime streams so far, but the future looks bright). Our faculty partners on Project Vietnam are also very interested in social interactions between and across student submissions, so the environment aggregates and synthesizes student work, and allows for student collaboration on some of their essays. This summer&#39;s Open Video Conference attempted to bring together interested stakeholders in this problem space. We have decided to start a conversation in their tent, holding our discussions with our perspective partners (including our collaboration with Georgetown&#39;s CNDLS on Project Rebirth) in public, in the hopes that perhaps other groups working on similar problems might stumble across our conversation and join in. At the Open Video Conference alot of people were talking about about standards for &#34;time-based metadata&#34; &#34;isochronic metadata&#34; or &#34;fine-grained metadata&#34;. There is a good chance that the web community will soon consolidate around these standards. In the meantime, our group has essentially bracketed the conversation around standards, and tried to proceed as-if that problem has already been solved, investigating what applications might look like once these standards are nailed down. While we need to track the standards conversations closely to leverage their gains, it is also exciting to bootstrap our way into the future of video annotations, by iterating over design approaches that presume these standard conversations are resolved.]",
   "tags": ["annotations", "django", "rebirth", "vietnam", "vital"]
},
{
   "url": "/articles/building-an-offline-web-app/",
   "title": "Building an offline web app",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[In March, CCNMTL shipped a laptop to a South African AIDS clinic as a part of a multimedia health-care intervention. We&#39;re not that experienced with desktop application development, so the main discussion was how do we bundle a web application on a stand-alone laptop with no connection to the Internet. The first proposal was to run a virtual machine (Xen or VMware) which would run the web server on the Windows desktop. I was less sanguine about diagnosing problems with a web server across continents and timezones, and looked for a way to store state information from static web pages. Firefox&#39;s DOM Storage was close to a HTML5 standard (now finally implemented in Firefox 3.5), and seemed to work with URLs visited as &#34;file://localhost/C:/...&#34; so this made the following process possible: Put static HTML files on the laptop All state is stored by the browser (in a file called webappsstore.sqlite) All application storage is accessed and modified by javascript (see code) Login state uses sessionStorage which works similarly but disappears after the browser closes (like a session cookie) Instead of supporting a virtualization and web server stack, all that&#39;s left to support is the browser--something very familiar to all computer users by now. It&#39;s worked out great. I should note that our application is not secure from a javascript hacker who has access to the computer--they could access and change all account information on our system. Fortunately, that&#39;s not an attack vector we&#39;re worried about. OK, there&#39;s a dirty secret behind my not posting about this previously--it no longer works! There&#39;s a laptop in a South African clinic that&#39;s not getting any Firefox updates, security or otherwise, and that&#39;s a very good thing. Now, it seems, all browsers, remove the &#39;localhost&#39; from file:// URLs. The new HTML5 standard localStorage does not work for local files, and the deprecated globalStorage[hostname] doesn&#39;t work without a hostname! HTML5 taketh away, but it giveth ath well. Instead of relying on file:// URLs, in the future we can label our site as an offline resource and then use the now standardized and implemented localStorage. The one issue with this future approach is if we need to update the application while it&#39;s in the field. We haven&#39;t needed to do that on this project, but it&#39;s a comfort to know that if they discovered a critical bug, we could email them a single HTML file to replace, and the computer running the application does not need to be connected (to anything other than the USB key the new file is on). I sent our use cases for localStorage over to the HTML5 mailing list, but there&#39;s still work on the standards side and for the browser vendors.]",
   "tags": ["firefox", "html5", "javascript", "masivukeni", "smart"]
},
{
   "url": "/articles/openid-at-columbia/",
   "title": "Prototype for columbia.edu as an OpenID Provider",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[OpenID is an increasingly popular universal sign-on mechanism on the web. Google, Facebook, LiveJournal, even Sears&#39; online store are supporting it. We can, in theory, adapt Columbia logins to be an OpenID provider. This would allow members of the Columbia community to login to other sites which accept OpenID with their Columbia UNIs. At Columbia, CUIT provides a single sign-on mechanism for services within the university called Wind , which is based on the more ubiquitous CAS . One problem with depending on Columbia-only authentication/authorization is that it makes it awkward, if not impossible, for students or faculty to work together with non-Columbia affiliates in the same protected environments. Guest lecturers can&#39;t access the course materials. Researchers have difficulty collaborating across institutions. The solution is to use a broader authentication method. Shibboleth is one that has been baking for a long time within the academic world, but it seems like OpenID provides many similar features and will allow our community to interface on more popular websites, too. It&#39;s not available yet; we still need to talk to the folks at CUIT about why it&#39;s a good idea. However, I&#39;ve written an implementation that should make the following scenario possible: For any service on the Internet, you should be able to type into an OpenID login &#39;columbia.edu.&#39; This will shepherd you to a Wind login, and then Columbia will authenticate you with the site. No saved passwords and, if you choose, your name and email will automatically be sent to them. The system, as implemented, would also let you login as an anonymous Columbia student or faculty member. Sometimes you don&#39;t want a site to know your name or even be able to match your login with other sites you&#39;ve logged in to the same way. Note, however, that the Columbia IT department could figure out who you are--you&#39;d be trusting their servers after all. Some tech details: I used the mostly friendly JanRain php-openid library OpenID is pretty complicated, and it took a bit of time before I started thinking my confusion was related to a bug in the code, rather than my confusion with the spec. Overall, though, the example server was a great place to start. One gotcha I encountered toward the end of the implementation: I was getting an error when testing at openidenabled.com &#34;OpenID authentication failed: No matching endpoint found after discovering [my OpenID].&#34; It turned out that the Relying Party (the server for which you login), on the last step, queries the user&#39;s identity_url to check that it trusts the Provider (the server providing the login). When I checked what my provider was doing, it was serving the wrong information in that case--easily broken, but easily fixed with some better htaccess rules. All the code is at http://github.com/ccnmtl/openid-wind-bridge]",
   "tags": ["login", "openid", "opensource-contrib", "wind"]
},
{
   "url": "/articles/why-ccnmtl-developers-like-firefox/",
   "title": "Why CCNMTL Developers like Firefox",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[Firefox 3.5 is just released today. Download it now! Especially for our more public projects, we aim to support all of the most popular web browsers. However, our favorite browser is Firefox. When development time is short, or we have a controlled environment, we will default to requiring Firefox. Why? Generally many of our projects go like this. We develop an application in a couple of weeks or months. It works perfectly in Firefox. Making it work in Safari is often a small tweak, at most, taking an hour or so. But for Internet Explorer, we often need a full extra week or more to solve all the program and style problems that appear. As developers, Firefox has some incredible extensions, without which modern web development would be impossible. The most common ones we use are Firebug , LiveHTTPHeaders , and WebDeveloper . For users, Firefox is also the best browser. In academic life we can use powerful extensions like Zotero . Because it&#39;s fast , we can design more demanding applications on the web. Because it supports SVG, Canvas and the video tag we can make more powerful applications for analyzing images and video.]",
   "tags": ["firefox"]
},
{
   "url": "/articles/video-goes-native-sfw/",
   "title": "Video Goes Native (SFW)",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[There is alot of buzz right now around the latest version of Firefox which finally implemented the native &amp;lt;video&amp;gt; tag specified in html5 . These developments were a hot topic at the Open Video Conference, which was about a whole lot more than just the video tag, but the timing was really perfect. For more about the OVC, see Jonah and Mark Phillipson&#39;s reports, as well as the announcement of our Open Sourcing of VITAL . The introduction of the &amp;lt;video&amp;gt; tag is a pretty big deal. Up until now, to display video in a browser, you needed to use third party plugins embedded in object tags. This allowed for the video to be seen in the browser, but the video wasn&#39;t really part of the web page - it was trapped inside a box. Suddenly, all sorts of really amazing things can happen - all the tools and operations that browsers preform on elements of a page can now be applied to video. Video can be dynamically modified using javascript and css, filters and transformations can be applied, canvas and svg can be combined with video, etc etc. When I saw this in action it made me realize how much I had assumed and taken for granted about the constraints around web video. But don&#39;t take my word for it - check out some of these early demos: Firefox 3.5 Treats Videos Like Web Pages Making video a first class citizen of the Web These demos (these require installing a browser that supports the new stuff). Some of my favs: washing machine filter mashup There is an incredibly important part of this story that has everything in the world to do with open standards, accessibility, and codecs which are unencumbered by patents, but I will save those angles are for another time. Meanwhile, even the big corporate players are excited. YouTube and Daily Motion both have html5 pages up, and Daily Motion is even converting their catalog to the royalty-free format, Ogg Theora (this is also how wikipedia is encoding video). Don&#39;t get too excited yet though. MS still has no official plans to support the video tag in IE (though apple/safari/webkit does - but only with quicktime codecs). Welcome to the Multimedia Wars of late Oughts. The revolution will definitely be an element of the Dom.]",
   "tags": ["conference", "html5", "openvideo"]
},
{
   "url": "/articles/google-geo-hackathon/",
   "title": "Google geo hackathon",
   "author": "Eddie Rubeiz",
   "date": "",
   "content": "[Jonah told me he had a spare ticket to yesterday&#39;s geo API hackathon at the Google headquarters last Thursday afternoon. I&#39;d long been curious to see the fabled interior of the New York Googleplex, and the event also offered a chance to catch up with how NYC developers have been using with geospatial data. New York is, after all, a location-obsessed town. Jonah told me he had a spare ticket to yesterday&#39;s &#34;geo API hackathon&#34; at the NYC Google headquarters last Thursday afternoon. I&#39;d long been curious to see the fabled interior of the New York Googleplex, and the event also offered a chance to catch up with how NYC developers have been using with geospatial data. New York is, after all, a location-obsessed town. Mano Marks and Roman Nurik, who work on the geo api team, kicked off the event with a brief presentation. Their efforts currently focus on integrating Google Maps and Google Earth; one recent milestone is the introduction of the Google Earth browser plugin. Widespread access Google Earth, in all of its 3-D glory, from a browser - even one on a mobile device-- makes possible projects like this map of a college library in Singapore, in which a call number brings up the book&#39;s shelf location in a 3-D model of the building. I spent the afternoon running through a couple of tutorials for the Google Maps API, which I hadn&#39;t used for some time and has apparently gotten easier to use. I sat with a number of developers from the New York Times, who&#39;ve been doing great work adding Times articles to G. Earth by tagging them with location data. I even got to meet Matthew Bloch, the genius behind the Times&#39; Electoral Explorer, which we all spent Election Night obsessing over. See his site, maps.grammata.com, for a number of other intriguing mashups. Other guests presented in the evening. It&#39;s gradually getting easier to track and plot ordinary cell phones&#39; locations, and several presenters were using both Maps and Earth to help analyze this data. Steve Bull is using this development to come up with some interesting games, which is an intriguing departure from the obvious applications of surveillance and marketing. Others were exploring the possibility of extracting aggregate information about the schedules of commuters who volunteered to share the whereabouts of their cellphones over a several-week period. I can certainly imagine a social science class sending out a dozen students, each with a cellphone broadcasting its coordinates, on various &#34;missions&#34;, and then gathering to analyze the resulting data. In short, the growing availability of geographical data, the increasing cleverness of the software that allows us to visualize it, and the ever-shrinking size of the gadgets that make it available to us, have only begun to change the way we live in, and study, this city.]",
   "tags": ["geo", "geography", "gis", "google", "sprint"]
},
{
   "url": "/articles/technical-justice-and-calculated-risks/",
   "title": "Technical Justice &amp; Calculated Risks",
   "author": "Jonah Bossewitch",
   "date": "",
   "content": "[The Collateral Consequences Calculator is premiering this week at the New York State Bar Association&#39;s annual meeting, marking the culmination of a 2 year development effort. The motivation and curricular goals driving this project are described in our portfolio. In addition to the formidable educational and logistical challenges, this project also presented some very unique technical challenges, which are worth documenting and celebrating. CCNMTL&#39;s primary mission is educational, and our design research is usually focused on pedagogy and improving the user experience, rather than infrastructure or basic research. Typically, we attempt to apply well understood software solutions to educational contexts and improve the experience around a well understood technical problem. While our developers dance on the cutting edge of enterprise solutions, it is difficult and risky to work on problems that computer scientists still consider &#39;hard&#39;. The Collateral Consequences Calculator is one of the more technically aggressive projects we have embarked on. The project has gone through various stages of complexity and sophistication, but at its core, we were asked to model the Law within Code. Since the Law is expressed using natural human language, this problem falls within the domain of artificial intelligence. We were also tasked with the development of an active model of the Law - a system that could make inferences about the collateral consequences triggered by individuals matching certain profiles, or a system that could guide the user through various scenarios using a series of wizard-like prompts. In the field of artificial intelligence these kinds of systems are known as expert systems and are notoriously difficult to develop and support. Our problem was particularly hairy, since the legal knowledge is highly specialized, very precise, and changes frequently. We needed a solution that accommodated expert verification, accuracy, and evolution over time. For lawyers and law students to participate in the creation and maintenance of this system, the representation of the law needed to be distinct from the code. As we researched this problem, we discovered there were some emerging standards around legal data exchange, and some computer scientists actively researching &#34;computational solutions enabling users to understand, utilize, exploit, and obey law&#34;. (see Computers and the Law ). These research efforts and fledgling standards had not yet been packaged for us in a ready-to-use format, although they did suggest and confirm some innovative approaches that Anders had already been actively investigating. Anders&#39; design took into account the precise allowances this project demanded, while remaining flexible enough to handle the ambiguity and inconsistency of the law. We engineered a system where the laws are encoded as a series of propositions, which in turn, are fed into an inference engine to calculate the consequences when various conditions are satisfied. We also built an administrative back-end for this system which, like a wiki, preserves a versioned history of this set of propositions. Our system is even intelligent enough to inform the user of which consequences might be affected by their updates to the rules. In the current system, the legal propositions are encoded using the n3 syntax, though even if we were to improve the editing interface, there really is no way around the challenge of translating the legal statutes into logical statements. The hard part is actually thinking so precisely about relationships and dependencies. This activity requires human judgement, and the creation of this initial body of knowledge was the main task for the students in the law clinic. Once the legal propositions have been entered into the system, they are processed by an inference engine written by the legendary Tim-Berners Lee (which Anders webified). On the production site, the entire space of consequences is pre-computed and cached, providing a snappy response to all requests. The production site makes this all look easy. We have not yet created an administrative interface that the lawyers were comfortable using, and many of the most sophisticated features are not visible in the existing tool. Gone are the client profiles, and the scenario wizards, though we do currently compare different convictions, and attempt to show the inferential chain of reasoning. The decoupling of the legal propositions from the underlying code was crucial for the success of this project. The essential task of modelling the law within software opens up a range of applications beyond the original calculator. We can easily imagine &#39;what-if&#39; simulations, where policy makers could experiment with changes of the law. Also, a system like this is the precondition for creating dynamic contracts or licenses on the fly (based on a human readable criterion, for example). Frighteningly, systems like these are also beginning to dispense &#39;technical justice&#39; - as computers are now calculating sentences in china and US drivers are being programmatically convicted of traffic violations.]",
   "tags": ["python", "semantic web"]
},
{
   "url": "/articles/visualizing-multi-player-simulation-roles/",
   "title": "Visualizing multi-player simulation rules",
   "author": "Schuyler Duveen",
   "date": "",
   "content": "[I&#39;ve been largely an outside observer to our Country X project. CountryX is a simulation used for International Policy students studying what scenarios lead to genocide. In the early stages, the rules of the &#39;game.&#39; converged on: 1. Four players, each in a specified role, starting with a watershed moment in an imaginary country. 2. The four roles are President, First-world Envoy, Regional Representative, and an Opposition Leader. (I immediately abbreviated these in my notes to &#39;P&#39;,&#39;E&#39;,&#39;R&#39;,&#39;O&#39;) 3. The game has four turns. In each turn, each of the four players decides on a context-specific list of three choices for their particular role. This is like playing a &#39;trick&#39; in Bridge or Hearts. When I first heard of the project, Jonah was working with the others on the project to specify the rules, for all of the contexts, in each of the states. He described an elaborate forest of paper Mark, our client, had on the walls of his office. How were we the programmers going to get him to describe the rules? The problem is that most rule-making devices, besides pure programming code, already have a domain and the conceptual framework of how outcomes result. That is, we can narrow the possibilities of an outcome based on some global rules or variables, which the author can specify. But we did not know the framework yet. So every turn had 3*3*3*3=81 (four players, three choices each) possible outcomes. This would diverge into a set of states for the next turn, and then for EACH state, 81 combinations would then diverge into the possible states for the next turn. I understand my first contribution, as suggesting that if there were no general rules, then the only choice was to give him a truth table of 81 rows, each listing the state of that combination for the players. Eventually, it turned out that there were 28 states (with 8 on the last turn), so poor Mark had to fill in 81*20=1620 rows. Impressively, Mark came through, and brought order to the possible outcomes (one of them genocide). Then Susan Dreher wowed us all with one of her first contributions to CCNMTL as bringing Mark&#39;s rules into a data model and environment for the game to be played. What were all of these rules though? We know they had been specified, but it was a gloss of 1620 rows. In the data, you can see some notes Mark makes along the margins, to keep some of it straight himself. But as outsiders, it was difficult to understand certain things. For example: 1. Game balance: how much do certain players affect the outcome from turn to turn? 2. Given Mark&#39;s rules, how can genocide be averted? Who is &#39;responsible&#39;? Susan used some sophisticated tools (e.g. GraphViz which produced this image) as a first pass, but mostly, it shows just how complicated the rules are. With a bit of time here and there, I was interested in finding a better solution. All the more because it seems like we might have more projects like this. There are still a couple ideas not-yet-implemented, but one that I think puts a little order to the madness is described below (If you&#39;re still reading, that is!) The idea is inspired by Karnaugh maps -which generally handle binary data Truth table data in two dimensions instead of the one long list of combinations, so it&#39;s easier to see some rules. XKCD has covered karnaugh maps a couple times The disadvantage is it takes a little longer to explain! Karnaugh maps have binary possibilities for each input. We have three possibilities for each of the four players. The real way to think of it is instead of an 81×1 list of rows we make it a 9×9 grid with two players on each side. Each of the roles have three choices. Here we see where to look for each role for their influence. Here are two examples presenting a single state&#39;s combinations in the two ways. There is a different color for each resulting state. First here&#39;s an example where the rules are moderately simple: At first it feels like, despite being a little more verbose, we can get the same understanding from looking at the linear version. Here&#39;s a more complicated set of rules. In the long version, you have to take an extra step of grouping the separated colors to figure out the rule. It takes a bit of practice, before one can start seeing and understanding the &#39;rules&#39; that these maps are explaining. We need to keep the full representation under wraps, so students don&#39;t use it to cheat. You can get the idea though, by examining this fake state sequence: The easiest way to get a sense of what it all means is to try and verbalize what the rules mean. For example, in Turn X, from the state &#39;Mediation,&#39; the TO grid shows thick horizontal bars. If we look to the KEY, the first two horizontal rows correspond to president decisions. Thus, we can start to tell the story from here that the major outcomes depend on the president&#39;s decision. The thin vertical horizontal bars in the bottom row (again by the KEY) correspond to the Opposition Leader&#39;s choice. Thus, we can tell the story that when the President chooses (3), the Opposition Leader can change the outcome. Besides that, from the &#39;Mediation&#39; state, no other player can influence the outcome. Work still to be done, should we decide this is a good direction, would be: 1. Make it more intuitive through interactive links. On mouse overs we can see what the choices are, and highlight the state in the next turn. 2. We could make this an authoring environment. However difficult, it should be easier than manually entering 1620 rows! 3. Another direction I&#39;m still thinking about is a waterfall-cascade view which may be explained or shown in another post.]",
   "tags": ["countryx", "simulations", "visualization"]
},
{
   "url": "/articles/musings-on-evil-ui/",
   "title": "Musings on Evil UI",
   "author": "Kathryn Hagan",
   "date": "",
   "content": "[I spent my weekend at HOPE, a hacker convention run by the good folks at 2600 Magazine. There were many interesting (and occasionally hilarious) talks on a variety of subjects, from the weirdest things you can send via the postal service to the basics of hacking DNA to a discussion of the GPS technology in NYC taxi cabs. One of my favorites was called &#34;Evil Interfaces: Violating the User&#34;. It was a taxonomy of manipulative UI methods that result from a situation where the goals of the designer are not the same as the goals of the user (for instance, they want you to pay attention to the advertising). The speaker, Gregory Conti, talked about how the most successful &#34;attacks&#34; result from an inversion of traditional UI design rules. So in some sense, it was a manual of &#34;what not to do&#34;: distract the user from the content with motion or color, disguise non-content as content, add extra barriers between the user and the content he wants to see (interstitial ads), exploit user errors, set bad defaults, etc. However, it gave me some food for thought about our projects. Now, you can truthfully say of all interfaces that, on some level, we use them to manipulate users, restricting their available courses of action and even pushing them in certain directions. It is also true that, as educators, our goals are not always the same as the user&#39;s; the pedagogical goals sometimes mean that we have to enforce a pattern of behavior that may not be the one the user has chosen, so we find ourselves having to compromise between pedagogy and usability. For example, in Business School case studies, we had a conversation about whether we should hide or disable menu items in order to prevent users from skipping ahead in the narrative structure. (We didn&#39;t, in the end, but it was almost decided the other way, and they may yet ask us for this, depending on how it is used in the classroom.) So the challenge for us is to design interfaces that set up this manipulation as an instructive relationship, not an adversarial one. While advertisers want to annoy users as little as possible but still force them to view the content, we want to force students to learn something (usually a specific something), without making it too obvious that we&#39;re pushing them in a certain direction, or setting them up for an &#34;aha&#34; moment. One of the precepts of Evil UI Design is forcing the users to do work (e.g. having to click through five screens of ads in order to get your free download), but we also want to make our users work, because we want them to learn something. One other way of thinking about this is what I wrote in my notebook at the talk: &#34;There&#39;s a thin line between scaffolding and walls.&#34; This is in no way any sort of criticism; actually, I think that we usually do a really good job of providing guidance without being too limiting. But it was interesting for me to think about, since I don&#39;t usually worry about the design issues behind a project.]",
   "tags": ["conference", "thelasthope", "ui"]
},
{
   "url": "/articles/unit-testing-the-law-or-how/",
   "title": "Unit testing the law or &#34;how I ended up with an n3 to sql compiler&#34;",
   "author": "Anders Pearson",
   "date": "",
   "content": "[The light at the end of the tunnel seems to actually be visible now for the Collateral Consequences project. There&#39;s still a formidable amount of manual data entry to be done, but we&#39;ve got a part-timer on that, so it&#39;s not my primary concern. The difficult work is now out of the way. If you&#39;ve been following the development at all, you know that we&#39;ve basically got a system worked out and running. There were plenty of tricky problems to solve to get there and I&#39;ve written about some of them here. The last big &#34;how on earth are we going to do that?&#34; problem that I kept putting off solving was verification. This project involves law and even though we have big disclaimers plastered over everything, none of us really wanted to release this application to be used without being really confident that it&#39;s not going to give people wrong results. The problem, as I&#39;ve explained before, is that the inherent complexity makes it really difficult to verify. You could change a single line of N3 code somewhere and potentially affect all the consequences on all the charges. It just isn&#39;t feasible to predict exactly what repercussions a change will have without running it for all the charges and manually inspecting the results. The first step towards coming up with a solution for the verification stuff was to acknowledge that part of the problem is insurmountable. To have confidence in the system, at some point, the lawyers need to just go through the list of charges one by one and check that the consequences are sane. There&#39;s just no way around it. Automated verification at least needs that baseline to exist. To at least make that step not too tedious for them, I added a &#39;snapshot&#39; functionality to the application which just goes down the list of charges and generates the consequences for each and every one of them and dumps them into a single (big) report that looks like this. As you can see there, we still have a lot of data entry work to do. Checking that report is still going to be a big job once it&#39;s all filled in, but it at least saves the law folks the tedium of going through every pulldown menu on the site and waiting for the results page to load for each, etc. That&#39;s probably fine to handle a single release of the application. But part of the purpose of the application in general and our approach of using N3 to model the logic behind the law and the consequences in particular is to keep it easy to update it in the future as the various laws and statutes change. If the entire thing has to be re-verified every time, that kind of kills those advantages. So the next step was realizing that once we have these snapshots, comparing them is relatively easy and we can produce reports showing just the charges whose consequences changed between two snapshots. Then, the verifiers can focus on just those ones and make sure that the changes are sane looking. It&#39;s not foolproof (eg, they still need to be able to recognize situations where charges should have been affected by a change but aren&#39;t showing up in the comparison), but it ought to save a lot of time over complete re-verifications. A sample of this report is here. It&#39;s pretty much the same as the full report, except it only shows the rows that changed between the two snapshots and displays the old version of the row under the new version so it&#39;s easy to see exactly what&#39;s different. In that sample report, the only real difference is that the old snapshot was missing the housing related consequences entirely. Having all of these snapshots archived and available I think will also help provide a sense of transparency on the project that will make it more appealing to the judges and lawyers that need to be convinced of its usefulness. They might not understand the technology that&#39;s generating the results, but being able to verify things for themselves should make them a little more comfortable with it. The snapshots, once a baseline is verified, will also serve as a really powerful unit test for the python side of the application. If I&#39;m changing the code that processes the N3, eg to improve performance, it will be really nice to compare before and after snapshots and make sure that I haven&#39;t inadvertently broken some particular edge case. That&#39;s the overview of how verification is being done. I&#39;d also like to mention a few technical details since I think there are some pretty cool things going on at that level. N3 is powerful and expressive, but it is a little sluggish. It&#39;s not too noticeable on a single query or two, but the full list of charges will be 500 or so. With just a fraction of those, it already takes about 3 minutes to calculate the consequences for all of them. So the first thing that I had to do with generating the reports was to figure out a way to cache them. We want to archive them anyway for comparison purposes, so some kind of serialization was necessary. What I did was to create a simple database schema for the charges and consequences and a corresponding SQLObject model. Since it&#39;s a turbogears project, the scaffolding was already in place for that and just wasn&#39;t being used. The only slightly unusual approach I took was to use SQLite as the underlying database instead of my usual PostgreSQL. SQLite is a simple embedded database. It stores all its data in a single file and runs within the python process instead of having a separate database process and communicating with sockets. It doesn&#39;t support the full set of SQL operations, but it supports enough for the simple purpose here. SQLite also doesn&#39;t deal that well with lots of concurrent reads and writes (since it doesn&#39;t have its own managing process, it has to do a lot of locking to prevent bad things from happening when multiple processes are accessing the file at the same time). But the approach here is that a snapshot is generated by processing all the N3 and dumping the output into the database in a batch operation that happens very rarely. Everything else would just be read-only. So concurrency just isn&#39;t an issue here. I started with SQLite just because it&#39;s so simple to set up. But once I had it, I realized that since the database was just a file, it could be checked into subversion and pushed to production along with the rest of the code. That meant that I could generate the snapshots on my dev machine and push them to production and they&#39;d be viewable there. Querying SQLite&#39;s also significantly faster than processing N3, so we can now use the most recent snapshot in the database as a cache. My plan is to have the app, when deployed to production, always just get results from the snapshot and really never even touch the N3 directly. So basically, what it&#39;s doing when I make a snapshot on my dev machine is compiling the N3 down to SQL and the production server will just be running off the compiled version. Hence the title of this post. The other interesting bit of technology used was Python&#39;s sets for doing the diffs. Sets were just an incredibly useful datastructure for doing this sort of comparison work. If you have sets consequences1 and consequences2, you can get a list of the consequences that are in 1 and not 2 just with: new = consequences1 - consequences2 the ones in 2 that aren&#39;t in one: removed = consequences2 - consequences1 and the ones that are in both with: intersection = consequences1 &amp;amp; consequences2 Much easier than looping over hash keys and doing a ton of comparisons and faster too.]",
   "tags": ["n3", "python", "semantic web"]
}]
